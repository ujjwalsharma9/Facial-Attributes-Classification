{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllTrain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q8nJAwjyz18-",
        "colab_type": "code",
        "outputId": "b60543cb-7f48-44ec-f3ed-c9c5843178c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5DiAvS07jPaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm *.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iqqQCfiWBw-G",
        "colab_type": "code",
        "outputId": "d8ecdeba-6889-4214-afbd-66b065856d92",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dade11a1-4e46-403e-a084-7f8614d2993e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dade11a1-4e46-403e-a084-7f8614d2993e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ujjwalsharma72\",\"key\":\"98a037e261e3febbb6f60b9c9070ae47\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "CAVIqiErB8hx",
        "colab_type": "code",
        "outputId": "51a7e8a8-8e53-46b4-c1fc-03b0b92715dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/78/832b9a9ec6b3baf8ec566e1f0a695f2fd08d2c94a6797257a106304bfc3c/kaggle-1.4.7.1.tar.gz (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.10.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.27.0)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->kaggle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 5.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
            "  Running setup.py bdist_wheel for kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/44/2c/df/22a6eeb780c36c28190faef6252b739fdc47145fd87a6642d4\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built kaggle python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, kaggle\n",
            "Successfully installed Unidecode-1.0.22 kaggle-1.4.7.1 python-slugify-1.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XkTuBOH8B-Mq",
        "colab_type": "code",
        "outputId": "5646c1ce-d3ac-4074-99ab-fae715721d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading celeba-dataset.zip to /content\n",
            "100% 1.21G/1.21G [00:11<00:00, 122MB/s] \n",
            "100% 1.21G/1.21G [00:11<00:00, 114MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dBmmiiWCCD-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/celeba-dataset.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "    \n",
        "with zipfile.ZipFile(\"/content/img_align_celeba.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f8H0UelvRaRj",
        "colab_type": "code",
        "outputId": "5d9e8713-dc20-4e5a-bf8d-c156659a65c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1926
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd  \n",
        "\n",
        "data=pd.read_csv('/content/list_attr_celeba.csv')\n",
        "\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>5_o_Clock_Shadow</th>\n",
              "      <th>Arched_Eyebrows</th>\n",
              "      <th>Attractive</th>\n",
              "      <th>Bags_Under_Eyes</th>\n",
              "      <th>Bald</th>\n",
              "      <th>Bangs</th>\n",
              "      <th>Big_Lips</th>\n",
              "      <th>Big_Nose</th>\n",
              "      <th>Black_Hair</th>\n",
              "      <th>...</th>\n",
              "      <th>Sideburns</th>\n",
              "      <th>Smiling</th>\n",
              "      <th>Straight_Hair</th>\n",
              "      <th>Wavy_Hair</th>\n",
              "      <th>Wearing_Earrings</th>\n",
              "      <th>Wearing_Hat</th>\n",
              "      <th>Wearing_Lipstick</th>\n",
              "      <th>Wearing_Necklace</th>\n",
              "      <th>Wearing_Necktie</th>\n",
              "      <th>Young</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000001.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000002.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000003.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000004.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000005.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>000006.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>000007.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000008.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>000009.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>000010.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>000011.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>000012.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>000013.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>000014.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>000015.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>000016.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>000017.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>000018.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>000019.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>000020.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>000021.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>000022.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>000023.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>000024.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>000025.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>000026.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>000027.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>000028.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>000029.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>000030.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202569</th>\n",
              "      <td>202570.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202570</th>\n",
              "      <td>202571.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202571</th>\n",
              "      <td>202572.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202572</th>\n",
              "      <td>202573.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202573</th>\n",
              "      <td>202574.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202574</th>\n",
              "      <td>202575.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202575</th>\n",
              "      <td>202576.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202576</th>\n",
              "      <td>202577.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202577</th>\n",
              "      <td>202578.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202578</th>\n",
              "      <td>202579.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202579</th>\n",
              "      <td>202580.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202580</th>\n",
              "      <td>202581.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202581</th>\n",
              "      <td>202582.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202582</th>\n",
              "      <td>202583.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202583</th>\n",
              "      <td>202584.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202584</th>\n",
              "      <td>202585.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202585</th>\n",
              "      <td>202586.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202586</th>\n",
              "      <td>202587.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202587</th>\n",
              "      <td>202588.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202588</th>\n",
              "      <td>202589.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202589</th>\n",
              "      <td>202590.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202590</th>\n",
              "      <td>202591.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202591</th>\n",
              "      <td>202592.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202592</th>\n",
              "      <td>202593.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202593</th>\n",
              "      <td>202594.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202594</th>\n",
              "      <td>202595.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202595</th>\n",
              "      <td>202596.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202596</th>\n",
              "      <td>202597.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202597</th>\n",
              "      <td>202598.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202598</th>\n",
              "      <td>202599.jpg</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>202599 rows × 41 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          image_id  5_o_Clock_Shadow  Arched_Eyebrows  Attractive  \\\n",
              "0       000001.jpg                -1                1           1   \n",
              "1       000002.jpg                -1               -1          -1   \n",
              "2       000003.jpg                -1               -1          -1   \n",
              "3       000004.jpg                -1               -1           1   \n",
              "4       000005.jpg                -1                1           1   \n",
              "5       000006.jpg                -1                1           1   \n",
              "6       000007.jpg                 1               -1           1   \n",
              "7       000008.jpg                 1                1          -1   \n",
              "8       000009.jpg                -1                1           1   \n",
              "9       000010.jpg                -1               -1           1   \n",
              "10      000011.jpg                -1               -1           1   \n",
              "11      000012.jpg                -1               -1           1   \n",
              "12      000013.jpg                -1               -1          -1   \n",
              "13      000014.jpg                -1                1          -1   \n",
              "14      000015.jpg                 1               -1          -1   \n",
              "15      000016.jpg                 1               -1          -1   \n",
              "16      000017.jpg                -1               -1          -1   \n",
              "17      000018.jpg                -1                1          -1   \n",
              "18      000019.jpg                -1                1           1   \n",
              "19      000020.jpg                -1               -1          -1   \n",
              "20      000021.jpg                -1               -1          -1   \n",
              "21      000022.jpg                -1                1          -1   \n",
              "22      000023.jpg                 1               -1           1   \n",
              "23      000024.jpg                -1                1           1   \n",
              "24      000025.jpg                 1               -1          -1   \n",
              "25      000026.jpg                -1               -1           1   \n",
              "26      000027.jpg                -1                1           1   \n",
              "27      000028.jpg                -1               -1           1   \n",
              "28      000029.jpg                -1                1           1   \n",
              "29      000030.jpg                -1               -1          -1   \n",
              "...            ...               ...              ...         ...   \n",
              "202569  202570.jpg                -1               -1          -1   \n",
              "202570  202571.jpg                -1                1          -1   \n",
              "202571  202572.jpg                -1                1           1   \n",
              "202572  202573.jpg                -1               -1          -1   \n",
              "202573  202574.jpg                -1                1           1   \n",
              "202574  202575.jpg                -1               -1          -1   \n",
              "202575  202576.jpg                -1                1           1   \n",
              "202576  202577.jpg                -1                1           1   \n",
              "202577  202578.jpg                -1                1           1   \n",
              "202578  202579.jpg                -1                1          -1   \n",
              "202579  202580.jpg                -1                1          -1   \n",
              "202580  202581.jpg                 1               -1          -1   \n",
              "202581  202582.jpg                -1               -1          -1   \n",
              "202582  202583.jpg                -1               -1          -1   \n",
              "202583  202584.jpg                -1               -1          -1   \n",
              "202584  202585.jpg                -1               -1          -1   \n",
              "202585  202586.jpg                -1               -1          -1   \n",
              "202586  202587.jpg                -1               -1           1   \n",
              "202587  202588.jpg                -1               -1          -1   \n",
              "202588  202589.jpg                 1               -1           1   \n",
              "202589  202590.jpg                -1               -1          -1   \n",
              "202590  202591.jpg                -1                1           1   \n",
              "202591  202592.jpg                -1                1          -1   \n",
              "202592  202593.jpg                -1               -1           1   \n",
              "202593  202594.jpg                -1                1           1   \n",
              "202594  202595.jpg                -1               -1           1   \n",
              "202595  202596.jpg                -1               -1          -1   \n",
              "202596  202597.jpg                -1               -1          -1   \n",
              "202597  202598.jpg                -1                1           1   \n",
              "202598  202599.jpg                -1                1           1   \n",
              "\n",
              "        Bags_Under_Eyes  Bald  Bangs  Big_Lips  Big_Nose  Black_Hair  ...    \\\n",
              "0                    -1    -1     -1        -1        -1          -1  ...     \n",
              "1                     1    -1     -1        -1         1          -1  ...     \n",
              "2                    -1    -1     -1         1        -1          -1  ...     \n",
              "3                    -1    -1     -1        -1        -1          -1  ...     \n",
              "4                    -1    -1     -1         1        -1          -1  ...     \n",
              "5                    -1    -1     -1         1        -1          -1  ...     \n",
              "6                     1    -1     -1         1         1           1  ...     \n",
              "7                     1    -1     -1         1        -1           1  ...     \n",
              "8                    -1    -1      1         1        -1          -1  ...     \n",
              "9                    -1    -1     -1        -1        -1          -1  ...     \n",
              "10                   -1    -1     -1        -1        -1           1  ...     \n",
              "11                    1    -1     -1        -1        -1           1  ...     \n",
              "12                   -1    -1     -1        -1        -1          -1  ...     \n",
              "13                   -1    -1     -1        -1         1           1  ...     \n",
              "14                    1    -1     -1        -1         1          -1  ...     \n",
              "15                    1    -1     -1        -1        -1          -1  ...     \n",
              "16                   -1    -1     -1        -1        -1           1  ...     \n",
              "17                   -1    -1     -1        -1         1          -1  ...     \n",
              "18                   -1    -1     -1        -1        -1          -1  ...     \n",
              "19                   -1    -1     -1        -1        -1           1  ...     \n",
              "20                   -1    -1     -1        -1         1          -1  ...     \n",
              "21                   -1    -1     -1         1        -1          -1  ...     \n",
              "22                   -1    -1     -1        -1         1          -1  ...     \n",
              "23                   -1    -1     -1         1        -1          -1  ...     \n",
              "24                    1    -1     -1        -1         1          -1  ...     \n",
              "25                   -1    -1     -1         1        -1          -1  ...     \n",
              "26                   -1    -1     -1         1        -1           1  ...     \n",
              "27                   -1    -1     -1        -1        -1          -1  ...     \n",
              "28                   -1    -1      1        -1        -1          -1  ...     \n",
              "29                    1    -1     -1        -1         1          -1  ...     \n",
              "...                 ...   ...    ...       ...       ...         ...  ...     \n",
              "202569               -1    -1     -1         1         1           1  ...     \n",
              "202570               -1    -1     -1        -1         1          -1  ...     \n",
              "202571               -1    -1     -1        -1         1           1  ...     \n",
              "202572               -1    -1     -1         1         1          -1  ...     \n",
              "202573               -1    -1     -1        -1        -1          -1  ...     \n",
              "202574               -1    -1     -1        -1        -1          -1  ...     \n",
              "202575               -1    -1     -1         1        -1          -1  ...     \n",
              "202576               -1    -1      1         1        -1           1  ...     \n",
              "202577               -1    -1     -1        -1        -1          -1  ...     \n",
              "202578                1    -1     -1         1         1           1  ...     \n",
              "202579               -1    -1      1         1        -1          -1  ...     \n",
              "202580                1    -1     -1        -1        -1          -1  ...     \n",
              "202581                1    -1      1         1         1          -1  ...     \n",
              "202582               -1    -1     -1        -1        -1          -1  ...     \n",
              "202583               -1    -1      1         1        -1          -1  ...     \n",
              "202584               -1    -1     -1        -1        -1          -1  ...     \n",
              "202585                1    -1     -1        -1        -1          -1  ...     \n",
              "202586               -1    -1     -1        -1        -1          -1  ...     \n",
              "202587               -1    -1     -1         1         1           1  ...     \n",
              "202588               -1    -1     -1        -1        -1          -1  ...     \n",
              "202589               -1    -1     -1        -1         1          -1  ...     \n",
              "202590               -1    -1     -1         1         1           1  ...     \n",
              "202591               -1    -1     -1        -1        -1          -1  ...     \n",
              "202592               -1    -1      1        -1        -1          -1  ...     \n",
              "202593               -1    -1     -1         1        -1          -1  ...     \n",
              "202594               -1    -1     -1         1        -1          -1  ...     \n",
              "202595               -1    -1      1         1        -1          -1  ...     \n",
              "202596               -1    -1     -1        -1        -1           1  ...     \n",
              "202597               -1    -1     -1         1        -1           1  ...     \n",
              "202598               -1    -1     -1        -1        -1          -1  ...     \n",
              "\n",
              "        Sideburns  Smiling  Straight_Hair  Wavy_Hair  Wearing_Earrings  \\\n",
              "0              -1        1              1         -1                 1   \n",
              "1              -1        1             -1         -1                -1   \n",
              "2              -1       -1             -1          1                -1   \n",
              "3              -1       -1              1         -1                 1   \n",
              "4              -1       -1             -1         -1                -1   \n",
              "5              -1       -1             -1          1                 1   \n",
              "6              -1       -1              1         -1                -1   \n",
              "7              -1       -1             -1         -1                -1   \n",
              "8              -1        1             -1         -1                 1   \n",
              "9              -1       -1             -1          1                -1   \n",
              "10             -1        1             -1         -1                -1   \n",
              "11             -1        1              1         -1                -1   \n",
              "12             -1        1              1         -1                -1   \n",
              "13             -1        1             -1         -1                -1   \n",
              "14             -1       -1              1         -1                -1   \n",
              "15             -1        1              1         -1                -1   \n",
              "16             -1        1              1         -1                -1   \n",
              "17             -1        1             -1          1                 1   \n",
              "18             -1       -1             -1          1                -1   \n",
              "19              1       -1             -1         -1                -1   \n",
              "20             -1        1              1         -1                -1   \n",
              "21             -1       -1             -1          1                -1   \n",
              "22             -1        1              1         -1                -1   \n",
              "23             -1       -1             -1         -1                 1   \n",
              "24             -1       -1             -1         -1                -1   \n",
              "25             -1       -1              1         -1                -1   \n",
              "26             -1        1             -1          1                -1   \n",
              "27             -1        1             -1          1                -1   \n",
              "28             -1        1             -1          1                 1   \n",
              "29             -1       -1             -1          1                -1   \n",
              "...           ...      ...            ...        ...               ...   \n",
              "202569         -1       -1             -1         -1                -1   \n",
              "202570         -1       -1             -1          1                 1   \n",
              "202571         -1       -1             -1          1                 1   \n",
              "202572         -1        1             -1          1                -1   \n",
              "202573         -1       -1             -1         -1                -1   \n",
              "202574         -1       -1             -1         -1                -1   \n",
              "202575         -1       -1              1         -1                -1   \n",
              "202576         -1        1             -1          1                -1   \n",
              "202577         -1        1             -1          1                 1   \n",
              "202578         -1        1             -1         -1                 1   \n",
              "202579         -1       -1             -1          1                -1   \n",
              "202580          1        1             -1          1                -1   \n",
              "202581         -1        1             -1         -1                 1   \n",
              "202582         -1        1             -1         -1                -1   \n",
              "202583         -1        1             -1          1                -1   \n",
              "202584         -1       -1             -1         -1                -1   \n",
              "202585         -1       -1             -1         -1                -1   \n",
              "202586         -1        1             -1          1                -1   \n",
              "202587         -1       -1             -1         -1                -1   \n",
              "202588         -1        1             -1         -1                -1   \n",
              "202589         -1       -1             -1         -1                -1   \n",
              "202590         -1        1              1         -1                 1   \n",
              "202591         -1        1             -1          1                -1   \n",
              "202592         -1        1             -1         -1                -1   \n",
              "202593         -1       -1             -1          1                 1   \n",
              "202594         -1       -1             -1         -1                -1   \n",
              "202595         -1        1              1         -1                -1   \n",
              "202596         -1        1             -1         -1                -1   \n",
              "202597         -1        1             -1          1                 1   \n",
              "202598         -1       -1             -1          1                -1   \n",
              "\n",
              "        Wearing_Hat  Wearing_Lipstick  Wearing_Necklace  Wearing_Necktie  \\\n",
              "0                -1                 1                -1               -1   \n",
              "1                -1                -1                -1               -1   \n",
              "2                -1                -1                -1               -1   \n",
              "3                -1                 1                 1               -1   \n",
              "4                -1                 1                -1               -1   \n",
              "5                -1                 1                -1               -1   \n",
              "6                -1                -1                -1               -1   \n",
              "7                -1                -1                -1               -1   \n",
              "8                -1                 1                -1               -1   \n",
              "9                -1                 1                -1               -1   \n",
              "10               -1                -1                -1               -1   \n",
              "11               -1                -1                -1               -1   \n",
              "12               -1                -1                -1               -1   \n",
              "13               -1                 1                 1               -1   \n",
              "14               -1                -1                -1                1   \n",
              "15               -1                -1                -1               -1   \n",
              "16               -1                -1                -1               -1   \n",
              "17               -1                 1                 1               -1   \n",
              "18               -1                 1                 1               -1   \n",
              "19               -1                -1                -1               -1   \n",
              "20               -1                -1                -1                1   \n",
              "21               -1                 1                 1               -1   \n",
              "22               -1                -1                -1               -1   \n",
              "23               -1                 1                 1               -1   \n",
              "24               -1                -1                -1               -1   \n",
              "25               -1                 1                -1               -1   \n",
              "26               -1                 1                -1               -1   \n",
              "27               -1                 1                -1               -1   \n",
              "28               -1                 1                -1               -1   \n",
              "29               -1                -1                -1               -1   \n",
              "...             ...               ...               ...              ...   \n",
              "202569           -1                -1                -1               -1   \n",
              "202570           -1                 1                -1               -1   \n",
              "202571           -1                 1                 1               -1   \n",
              "202572           -1                -1                -1               -1   \n",
              "202573           -1                 1                -1               -1   \n",
              "202574           -1                -1                -1               -1   \n",
              "202575           -1                 1                 1               -1   \n",
              "202576           -1                 1                 1               -1   \n",
              "202577           -1                 1                -1               -1   \n",
              "202578           -1                 1                -1               -1   \n",
              "202579           -1                 1                 1               -1   \n",
              "202580           -1                -1                -1                1   \n",
              "202581           -1                 1                -1               -1   \n",
              "202582           -1                -1                -1               -1   \n",
              "202583           -1                -1                -1               -1   \n",
              "202584           -1                -1                -1               -1   \n",
              "202585           -1                -1                -1                1   \n",
              "202586           -1                 1                -1               -1   \n",
              "202587           -1                -1                -1                1   \n",
              "202588            1                -1                -1               -1   \n",
              "202589           -1                -1                -1               -1   \n",
              "202590           -1                 1                 1               -1   \n",
              "202591           -1                 1                -1               -1   \n",
              "202592           -1                 1                -1               -1   \n",
              "202593           -1                 1                -1               -1   \n",
              "202594           -1                 1                -1               -1   \n",
              "202595           -1                -1                -1               -1   \n",
              "202596           -1                -1                -1               -1   \n",
              "202597           -1                 1                -1               -1   \n",
              "202598           -1                 1                -1               -1   \n",
              "\n",
              "        Young  \n",
              "0           1  \n",
              "1           1  \n",
              "2           1  \n",
              "3           1  \n",
              "4           1  \n",
              "5           1  \n",
              "6           1  \n",
              "7           1  \n",
              "8           1  \n",
              "9           1  \n",
              "10          1  \n",
              "11          1  \n",
              "12          1  \n",
              "13          1  \n",
              "14         -1  \n",
              "15          1  \n",
              "16          1  \n",
              "17         -1  \n",
              "18          1  \n",
              "19          1  \n",
              "20         -1  \n",
              "21          1  \n",
              "22          1  \n",
              "23          1  \n",
              "24          1  \n",
              "25          1  \n",
              "26          1  \n",
              "27          1  \n",
              "28          1  \n",
              "29         -1  \n",
              "...       ...  \n",
              "202569      1  \n",
              "202570      1  \n",
              "202571      1  \n",
              "202572     -1  \n",
              "202573      1  \n",
              "202574      1  \n",
              "202575      1  \n",
              "202576      1  \n",
              "202577      1  \n",
              "202578      1  \n",
              "202579      1  \n",
              "202580     -1  \n",
              "202581      1  \n",
              "202582      1  \n",
              "202583      1  \n",
              "202584      1  \n",
              "202585     -1  \n",
              "202586      1  \n",
              "202587      1  \n",
              "202588      1  \n",
              "202589     -1  \n",
              "202590      1  \n",
              "202591      1  \n",
              "202592      1  \n",
              "202593      1  \n",
              "202594      1  \n",
              "202595      1  \n",
              "202596      1  \n",
              "202597      1  \n",
              "202598      1  \n",
              "\n",
              "[202599 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "TghijPqRSd4a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "col_list=data.columns.tolist()\n",
        "col_list.remove('image_id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-qYiJtS-7q7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLhJomPZ_fQ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jc_KrFHS_sNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '104OtLwFHBhsRihqsrA1_cpkG8R7ZsWSB'})\n",
        "download.GetContentFile('train_data.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFKx43IeYY8B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('train_data.pkl','rb') as f: \n",
        "    train_files,train_labels = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-Qog8biZDrd",
        "colab_type": "code",
        "outputId": "2e19be09-11a3-4617-d773-df213f63bf2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_files[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "GDcG2LV7Ek3V",
        "colab_type": "code",
        "outputId": "c292b664-0dde-4b4b-eaa9-7830de613076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17812
        }
      },
      "cell_type": "code",
      "source": [
        "train_files[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['img_align_celeba/010005.jpg',\n",
              " 'img_align_celeba/010000.jpg',\n",
              " 'img_align_celeba/010033.jpg',\n",
              " 'img_align_celeba/010001.jpg',\n",
              " 'img_align_celeba/010038.jpg',\n",
              " 'img_align_celeba/010002.jpg',\n",
              " 'img_align_celeba/010039.jpg',\n",
              " 'img_align_celeba/010003.jpg',\n",
              " 'img_align_celeba/010046.jpg',\n",
              " 'img_align_celeba/010004.jpg',\n",
              " 'img_align_celeba/010052.jpg',\n",
              " 'img_align_celeba/010006.jpg',\n",
              " 'img_align_celeba/010056.jpg',\n",
              " 'img_align_celeba/010007.jpg',\n",
              " 'img_align_celeba/010058.jpg',\n",
              " 'img_align_celeba/010008.jpg',\n",
              " 'img_align_celeba/010063.jpg',\n",
              " 'img_align_celeba/010009.jpg',\n",
              " 'img_align_celeba/010069.jpg',\n",
              " 'img_align_celeba/010010.jpg',\n",
              " 'img_align_celeba/010072.jpg',\n",
              " 'img_align_celeba/010011.jpg',\n",
              " 'img_align_celeba/010083.jpg',\n",
              " 'img_align_celeba/010012.jpg',\n",
              " 'img_align_celeba/010087.jpg',\n",
              " 'img_align_celeba/010013.jpg',\n",
              " 'img_align_celeba/010097.jpg',\n",
              " 'img_align_celeba/010014.jpg',\n",
              " 'img_align_celeba/010100.jpg',\n",
              " 'img_align_celeba/010015.jpg',\n",
              " 'img_align_celeba/010102.jpg',\n",
              " 'img_align_celeba/010016.jpg',\n",
              " 'img_align_celeba/010117.jpg',\n",
              " 'img_align_celeba/010017.jpg',\n",
              " 'img_align_celeba/010133.jpg',\n",
              " 'img_align_celeba/010018.jpg',\n",
              " 'img_align_celeba/010139.jpg',\n",
              " 'img_align_celeba/010019.jpg',\n",
              " 'img_align_celeba/010141.jpg',\n",
              " 'img_align_celeba/010020.jpg',\n",
              " 'img_align_celeba/010146.jpg',\n",
              " 'img_align_celeba/010021.jpg',\n",
              " 'img_align_celeba/010147.jpg',\n",
              " 'img_align_celeba/010022.jpg',\n",
              " 'img_align_celeba/010157.jpg',\n",
              " 'img_align_celeba/010023.jpg',\n",
              " 'img_align_celeba/010168.jpg',\n",
              " 'img_align_celeba/010024.jpg',\n",
              " 'img_align_celeba/010176.jpg',\n",
              " 'img_align_celeba/010025.jpg',\n",
              " 'img_align_celeba/010198.jpg',\n",
              " 'img_align_celeba/010026.jpg',\n",
              " 'img_align_celeba/010226.jpg',\n",
              " 'img_align_celeba/010027.jpg',\n",
              " 'img_align_celeba/010232.jpg',\n",
              " 'img_align_celeba/010028.jpg',\n",
              " 'img_align_celeba/010246.jpg',\n",
              " 'img_align_celeba/010029.jpg',\n",
              " 'img_align_celeba/010253.jpg',\n",
              " 'img_align_celeba/010030.jpg',\n",
              " 'img_align_celeba/010260.jpg',\n",
              " 'img_align_celeba/010031.jpg',\n",
              " 'img_align_celeba/010262.jpg',\n",
              " 'img_align_celeba/010032.jpg',\n",
              " 'img_align_celeba/010266.jpg',\n",
              " 'img_align_celeba/010034.jpg',\n",
              " 'img_align_celeba/010279.jpg',\n",
              " 'img_align_celeba/010035.jpg',\n",
              " 'img_align_celeba/010281.jpg',\n",
              " 'img_align_celeba/010036.jpg',\n",
              " 'img_align_celeba/010286.jpg',\n",
              " 'img_align_celeba/010037.jpg',\n",
              " 'img_align_celeba/010309.jpg',\n",
              " 'img_align_celeba/010040.jpg',\n",
              " 'img_align_celeba/010315.jpg',\n",
              " 'img_align_celeba/010041.jpg',\n",
              " 'img_align_celeba/010321.jpg',\n",
              " 'img_align_celeba/010042.jpg',\n",
              " 'img_align_celeba/010328.jpg',\n",
              " 'img_align_celeba/010043.jpg',\n",
              " 'img_align_celeba/010333.jpg',\n",
              " 'img_align_celeba/010044.jpg',\n",
              " 'img_align_celeba/010336.jpg',\n",
              " 'img_align_celeba/010045.jpg',\n",
              " 'img_align_celeba/010375.jpg',\n",
              " 'img_align_celeba/010047.jpg',\n",
              " 'img_align_celeba/010390.jpg',\n",
              " 'img_align_celeba/010048.jpg',\n",
              " 'img_align_celeba/010396.jpg',\n",
              " 'img_align_celeba/010049.jpg',\n",
              " 'img_align_celeba/010411.jpg',\n",
              " 'img_align_celeba/010050.jpg',\n",
              " 'img_align_celeba/010418.jpg',\n",
              " 'img_align_celeba/010051.jpg',\n",
              " 'img_align_celeba/010437.jpg',\n",
              " 'img_align_celeba/010053.jpg',\n",
              " 'img_align_celeba/010438.jpg',\n",
              " 'img_align_celeba/010054.jpg',\n",
              " 'img_align_celeba/010446.jpg',\n",
              " 'img_align_celeba/010055.jpg',\n",
              " 'img_align_celeba/010449.jpg',\n",
              " 'img_align_celeba/010057.jpg',\n",
              " 'img_align_celeba/010458.jpg',\n",
              " 'img_align_celeba/010059.jpg',\n",
              " 'img_align_celeba/010476.jpg',\n",
              " 'img_align_celeba/010060.jpg',\n",
              " 'img_align_celeba/010485.jpg',\n",
              " 'img_align_celeba/010061.jpg',\n",
              " 'img_align_celeba/010490.jpg',\n",
              " 'img_align_celeba/010062.jpg',\n",
              " 'img_align_celeba/010491.jpg',\n",
              " 'img_align_celeba/010064.jpg',\n",
              " 'img_align_celeba/010493.jpg',\n",
              " 'img_align_celeba/010065.jpg',\n",
              " 'img_align_celeba/010505.jpg',\n",
              " 'img_align_celeba/010066.jpg',\n",
              " 'img_align_celeba/010506.jpg',\n",
              " 'img_align_celeba/010067.jpg',\n",
              " 'img_align_celeba/010508.jpg',\n",
              " 'img_align_celeba/010068.jpg',\n",
              " 'img_align_celeba/010510.jpg',\n",
              " 'img_align_celeba/010070.jpg',\n",
              " 'img_align_celeba/010518.jpg',\n",
              " 'img_align_celeba/010071.jpg',\n",
              " 'img_align_celeba/010537.jpg',\n",
              " 'img_align_celeba/010073.jpg',\n",
              " 'img_align_celeba/010549.jpg',\n",
              " 'img_align_celeba/010074.jpg',\n",
              " 'img_align_celeba/010550.jpg',\n",
              " 'img_align_celeba/010075.jpg',\n",
              " 'img_align_celeba/010580.jpg',\n",
              " 'img_align_celeba/010076.jpg',\n",
              " 'img_align_celeba/010581.jpg',\n",
              " 'img_align_celeba/010077.jpg',\n",
              " 'img_align_celeba/010591.jpg',\n",
              " 'img_align_celeba/010078.jpg',\n",
              " 'img_align_celeba/010597.jpg',\n",
              " 'img_align_celeba/010079.jpg',\n",
              " 'img_align_celeba/010601.jpg',\n",
              " 'img_align_celeba/010080.jpg',\n",
              " 'img_align_celeba/010606.jpg',\n",
              " 'img_align_celeba/010081.jpg',\n",
              " 'img_align_celeba/010625.jpg',\n",
              " 'img_align_celeba/010082.jpg',\n",
              " 'img_align_celeba/010631.jpg',\n",
              " 'img_align_celeba/010084.jpg',\n",
              " 'img_align_celeba/010632.jpg',\n",
              " 'img_align_celeba/010085.jpg',\n",
              " 'img_align_celeba/010641.jpg',\n",
              " 'img_align_celeba/010086.jpg',\n",
              " 'img_align_celeba/010646.jpg',\n",
              " 'img_align_celeba/010088.jpg',\n",
              " 'img_align_celeba/010652.jpg',\n",
              " 'img_align_celeba/010089.jpg',\n",
              " 'img_align_celeba/010653.jpg',\n",
              " 'img_align_celeba/010090.jpg',\n",
              " 'img_align_celeba/010658.jpg',\n",
              " 'img_align_celeba/010091.jpg',\n",
              " 'img_align_celeba/010663.jpg',\n",
              " 'img_align_celeba/010092.jpg',\n",
              " 'img_align_celeba/010677.jpg',\n",
              " 'img_align_celeba/010093.jpg',\n",
              " 'img_align_celeba/010709.jpg',\n",
              " 'img_align_celeba/010094.jpg',\n",
              " 'img_align_celeba/010717.jpg',\n",
              " 'img_align_celeba/010095.jpg',\n",
              " 'img_align_celeba/010723.jpg',\n",
              " 'img_align_celeba/010096.jpg',\n",
              " 'img_align_celeba/010725.jpg',\n",
              " 'img_align_celeba/010098.jpg',\n",
              " 'img_align_celeba/010727.jpg',\n",
              " 'img_align_celeba/010099.jpg',\n",
              " 'img_align_celeba/010733.jpg',\n",
              " 'img_align_celeba/010101.jpg',\n",
              " 'img_align_celeba/010738.jpg',\n",
              " 'img_align_celeba/010103.jpg',\n",
              " 'img_align_celeba/010747.jpg',\n",
              " 'img_align_celeba/010104.jpg',\n",
              " 'img_align_celeba/010757.jpg',\n",
              " 'img_align_celeba/010105.jpg',\n",
              " 'img_align_celeba/010771.jpg',\n",
              " 'img_align_celeba/010106.jpg',\n",
              " 'img_align_celeba/010778.jpg',\n",
              " 'img_align_celeba/010107.jpg',\n",
              " 'img_align_celeba/010814.jpg',\n",
              " 'img_align_celeba/010108.jpg',\n",
              " 'img_align_celeba/010818.jpg',\n",
              " 'img_align_celeba/010109.jpg',\n",
              " 'img_align_celeba/010832.jpg',\n",
              " 'img_align_celeba/010110.jpg',\n",
              " 'img_align_celeba/010851.jpg',\n",
              " 'img_align_celeba/010111.jpg',\n",
              " 'img_align_celeba/010855.jpg',\n",
              " 'img_align_celeba/010112.jpg',\n",
              " 'img_align_celeba/010858.jpg',\n",
              " 'img_align_celeba/010113.jpg',\n",
              " 'img_align_celeba/010938.jpg',\n",
              " 'img_align_celeba/010114.jpg',\n",
              " 'img_align_celeba/010947.jpg',\n",
              " 'img_align_celeba/010115.jpg',\n",
              " 'img_align_celeba/010952.jpg',\n",
              " 'img_align_celeba/010116.jpg',\n",
              " 'img_align_celeba/010973.jpg',\n",
              " 'img_align_celeba/010118.jpg',\n",
              " 'img_align_celeba/010974.jpg',\n",
              " 'img_align_celeba/010119.jpg',\n",
              " 'img_align_celeba/010979.jpg',\n",
              " 'img_align_celeba/010120.jpg',\n",
              " 'img_align_celeba/010980.jpg',\n",
              " 'img_align_celeba/010121.jpg',\n",
              " 'img_align_celeba/010985.jpg',\n",
              " 'img_align_celeba/010122.jpg',\n",
              " 'img_align_celeba/010988.jpg',\n",
              " 'img_align_celeba/010123.jpg',\n",
              " 'img_align_celeba/010991.jpg',\n",
              " 'img_align_celeba/010124.jpg',\n",
              " 'img_align_celeba/010993.jpg',\n",
              " 'img_align_celeba/010125.jpg',\n",
              " 'img_align_celeba/010996.jpg',\n",
              " 'img_align_celeba/010126.jpg',\n",
              " 'img_align_celeba/011002.jpg',\n",
              " 'img_align_celeba/010127.jpg',\n",
              " 'img_align_celeba/011004.jpg',\n",
              " 'img_align_celeba/010128.jpg',\n",
              " 'img_align_celeba/011006.jpg',\n",
              " 'img_align_celeba/010129.jpg',\n",
              " 'img_align_celeba/011008.jpg',\n",
              " 'img_align_celeba/010130.jpg',\n",
              " 'img_align_celeba/011022.jpg',\n",
              " 'img_align_celeba/010131.jpg',\n",
              " 'img_align_celeba/011027.jpg',\n",
              " 'img_align_celeba/010132.jpg',\n",
              " 'img_align_celeba/011034.jpg',\n",
              " 'img_align_celeba/010134.jpg',\n",
              " 'img_align_celeba/011039.jpg',\n",
              " 'img_align_celeba/010135.jpg',\n",
              " 'img_align_celeba/011048.jpg',\n",
              " 'img_align_celeba/010136.jpg',\n",
              " 'img_align_celeba/011052.jpg',\n",
              " 'img_align_celeba/010137.jpg',\n",
              " 'img_align_celeba/011068.jpg',\n",
              " 'img_align_celeba/010138.jpg',\n",
              " 'img_align_celeba/011083.jpg',\n",
              " 'img_align_celeba/010140.jpg',\n",
              " 'img_align_celeba/011093.jpg',\n",
              " 'img_align_celeba/010142.jpg',\n",
              " 'img_align_celeba/011106.jpg',\n",
              " 'img_align_celeba/010143.jpg',\n",
              " 'img_align_celeba/011107.jpg',\n",
              " 'img_align_celeba/010144.jpg',\n",
              " 'img_align_celeba/011121.jpg',\n",
              " 'img_align_celeba/010145.jpg',\n",
              " 'img_align_celeba/011122.jpg',\n",
              " 'img_align_celeba/010148.jpg',\n",
              " 'img_align_celeba/011133.jpg',\n",
              " 'img_align_celeba/010149.jpg',\n",
              " 'img_align_celeba/011141.jpg',\n",
              " 'img_align_celeba/010150.jpg',\n",
              " 'img_align_celeba/011145.jpg',\n",
              " 'img_align_celeba/010151.jpg',\n",
              " 'img_align_celeba/011159.jpg',\n",
              " 'img_align_celeba/010152.jpg',\n",
              " 'img_align_celeba/011167.jpg',\n",
              " 'img_align_celeba/010153.jpg',\n",
              " 'img_align_celeba/011174.jpg',\n",
              " 'img_align_celeba/010154.jpg',\n",
              " 'img_align_celeba/011183.jpg',\n",
              " 'img_align_celeba/010155.jpg',\n",
              " 'img_align_celeba/011196.jpg',\n",
              " 'img_align_celeba/010156.jpg',\n",
              " 'img_align_celeba/011200.jpg',\n",
              " 'img_align_celeba/010158.jpg',\n",
              " 'img_align_celeba/011201.jpg',\n",
              " 'img_align_celeba/010159.jpg',\n",
              " 'img_align_celeba/011202.jpg',\n",
              " 'img_align_celeba/010160.jpg',\n",
              " 'img_align_celeba/011217.jpg',\n",
              " 'img_align_celeba/010161.jpg',\n",
              " 'img_align_celeba/011222.jpg',\n",
              " 'img_align_celeba/010162.jpg',\n",
              " 'img_align_celeba/011224.jpg',\n",
              " 'img_align_celeba/010163.jpg',\n",
              " 'img_align_celeba/011227.jpg',\n",
              " 'img_align_celeba/010164.jpg',\n",
              " 'img_align_celeba/011228.jpg',\n",
              " 'img_align_celeba/010165.jpg',\n",
              " 'img_align_celeba/011252.jpg',\n",
              " 'img_align_celeba/010166.jpg',\n",
              " 'img_align_celeba/011256.jpg',\n",
              " 'img_align_celeba/010167.jpg',\n",
              " 'img_align_celeba/011266.jpg',\n",
              " 'img_align_celeba/010169.jpg',\n",
              " 'img_align_celeba/011278.jpg',\n",
              " 'img_align_celeba/010170.jpg',\n",
              " 'img_align_celeba/011279.jpg',\n",
              " 'img_align_celeba/010171.jpg',\n",
              " 'img_align_celeba/011281.jpg',\n",
              " 'img_align_celeba/010172.jpg',\n",
              " 'img_align_celeba/011284.jpg',\n",
              " 'img_align_celeba/010173.jpg',\n",
              " 'img_align_celeba/011290.jpg',\n",
              " 'img_align_celeba/010174.jpg',\n",
              " 'img_align_celeba/011304.jpg',\n",
              " 'img_align_celeba/010175.jpg',\n",
              " 'img_align_celeba/011314.jpg',\n",
              " 'img_align_celeba/010177.jpg',\n",
              " 'img_align_celeba/011316.jpg',\n",
              " 'img_align_celeba/010178.jpg',\n",
              " 'img_align_celeba/011320.jpg',\n",
              " 'img_align_celeba/010179.jpg',\n",
              " 'img_align_celeba/011325.jpg',\n",
              " 'img_align_celeba/010180.jpg',\n",
              " 'img_align_celeba/011337.jpg',\n",
              " 'img_align_celeba/010181.jpg',\n",
              " 'img_align_celeba/011338.jpg',\n",
              " 'img_align_celeba/010182.jpg',\n",
              " 'img_align_celeba/011341.jpg',\n",
              " 'img_align_celeba/010183.jpg',\n",
              " 'img_align_celeba/011344.jpg',\n",
              " 'img_align_celeba/010184.jpg',\n",
              " 'img_align_celeba/011346.jpg',\n",
              " 'img_align_celeba/010185.jpg',\n",
              " 'img_align_celeba/011352.jpg',\n",
              " 'img_align_celeba/010186.jpg',\n",
              " 'img_align_celeba/011357.jpg',\n",
              " 'img_align_celeba/010187.jpg',\n",
              " 'img_align_celeba/011358.jpg',\n",
              " 'img_align_celeba/010188.jpg',\n",
              " 'img_align_celeba/011377.jpg',\n",
              " 'img_align_celeba/010189.jpg',\n",
              " 'img_align_celeba/011381.jpg',\n",
              " 'img_align_celeba/010190.jpg',\n",
              " 'img_align_celeba/011389.jpg',\n",
              " 'img_align_celeba/010191.jpg',\n",
              " 'img_align_celeba/011392.jpg',\n",
              " 'img_align_celeba/010192.jpg',\n",
              " 'img_align_celeba/011398.jpg',\n",
              " 'img_align_celeba/010193.jpg',\n",
              " 'img_align_celeba/011403.jpg',\n",
              " 'img_align_celeba/010194.jpg',\n",
              " 'img_align_celeba/011419.jpg',\n",
              " 'img_align_celeba/010195.jpg',\n",
              " 'img_align_celeba/011420.jpg',\n",
              " 'img_align_celeba/010196.jpg',\n",
              " 'img_align_celeba/011423.jpg',\n",
              " 'img_align_celeba/010197.jpg',\n",
              " 'img_align_celeba/011432.jpg',\n",
              " 'img_align_celeba/010199.jpg',\n",
              " 'img_align_celeba/011435.jpg',\n",
              " 'img_align_celeba/010200.jpg',\n",
              " 'img_align_celeba/011447.jpg',\n",
              " 'img_align_celeba/010201.jpg',\n",
              " 'img_align_celeba/011472.jpg',\n",
              " 'img_align_celeba/010202.jpg',\n",
              " 'img_align_celeba/011511.jpg',\n",
              " 'img_align_celeba/010203.jpg',\n",
              " 'img_align_celeba/011514.jpg',\n",
              " 'img_align_celeba/010204.jpg',\n",
              " 'img_align_celeba/011516.jpg',\n",
              " 'img_align_celeba/010205.jpg',\n",
              " 'img_align_celeba/011528.jpg',\n",
              " 'img_align_celeba/010206.jpg',\n",
              " 'img_align_celeba/011532.jpg',\n",
              " 'img_align_celeba/010207.jpg',\n",
              " 'img_align_celeba/011533.jpg',\n",
              " 'img_align_celeba/010208.jpg',\n",
              " 'img_align_celeba/011535.jpg',\n",
              " 'img_align_celeba/010209.jpg',\n",
              " 'img_align_celeba/011553.jpg',\n",
              " 'img_align_celeba/010210.jpg',\n",
              " 'img_align_celeba/011561.jpg',\n",
              " 'img_align_celeba/010211.jpg',\n",
              " 'img_align_celeba/011569.jpg',\n",
              " 'img_align_celeba/010212.jpg',\n",
              " 'img_align_celeba/011578.jpg',\n",
              " 'img_align_celeba/010213.jpg',\n",
              " 'img_align_celeba/011604.jpg',\n",
              " 'img_align_celeba/010214.jpg',\n",
              " 'img_align_celeba/011608.jpg',\n",
              " 'img_align_celeba/010215.jpg',\n",
              " 'img_align_celeba/011615.jpg',\n",
              " 'img_align_celeba/010216.jpg',\n",
              " 'img_align_celeba/011622.jpg',\n",
              " 'img_align_celeba/010217.jpg',\n",
              " 'img_align_celeba/011629.jpg',\n",
              " 'img_align_celeba/010218.jpg',\n",
              " 'img_align_celeba/011631.jpg',\n",
              " 'img_align_celeba/010219.jpg',\n",
              " 'img_align_celeba/011632.jpg',\n",
              " 'img_align_celeba/010220.jpg',\n",
              " 'img_align_celeba/011639.jpg',\n",
              " 'img_align_celeba/010221.jpg',\n",
              " 'img_align_celeba/011640.jpg',\n",
              " 'img_align_celeba/010222.jpg',\n",
              " 'img_align_celeba/011643.jpg',\n",
              " 'img_align_celeba/010223.jpg',\n",
              " 'img_align_celeba/011645.jpg',\n",
              " 'img_align_celeba/010224.jpg',\n",
              " 'img_align_celeba/011657.jpg',\n",
              " 'img_align_celeba/010225.jpg',\n",
              " 'img_align_celeba/011662.jpg',\n",
              " 'img_align_celeba/010227.jpg',\n",
              " 'img_align_celeba/011666.jpg',\n",
              " 'img_align_celeba/010228.jpg',\n",
              " 'img_align_celeba/011667.jpg',\n",
              " 'img_align_celeba/010229.jpg',\n",
              " 'img_align_celeba/011678.jpg',\n",
              " 'img_align_celeba/010230.jpg',\n",
              " 'img_align_celeba/011706.jpg',\n",
              " 'img_align_celeba/010231.jpg',\n",
              " 'img_align_celeba/011708.jpg',\n",
              " 'img_align_celeba/010233.jpg',\n",
              " 'img_align_celeba/011717.jpg',\n",
              " 'img_align_celeba/010234.jpg',\n",
              " 'img_align_celeba/011733.jpg',\n",
              " 'img_align_celeba/010235.jpg',\n",
              " 'img_align_celeba/011741.jpg',\n",
              " 'img_align_celeba/010236.jpg',\n",
              " 'img_align_celeba/011758.jpg',\n",
              " 'img_align_celeba/010237.jpg',\n",
              " 'img_align_celeba/011761.jpg',\n",
              " 'img_align_celeba/010238.jpg',\n",
              " 'img_align_celeba/011767.jpg',\n",
              " 'img_align_celeba/010239.jpg',\n",
              " 'img_align_celeba/011773.jpg',\n",
              " 'img_align_celeba/010240.jpg',\n",
              " 'img_align_celeba/011783.jpg',\n",
              " 'img_align_celeba/010241.jpg',\n",
              " 'img_align_celeba/011794.jpg',\n",
              " 'img_align_celeba/010242.jpg',\n",
              " 'img_align_celeba/011796.jpg',\n",
              " 'img_align_celeba/010243.jpg',\n",
              " 'img_align_celeba/011797.jpg',\n",
              " 'img_align_celeba/010244.jpg',\n",
              " 'img_align_celeba/011799.jpg',\n",
              " 'img_align_celeba/010245.jpg',\n",
              " 'img_align_celeba/011810.jpg',\n",
              " 'img_align_celeba/010247.jpg',\n",
              " 'img_align_celeba/011834.jpg',\n",
              " 'img_align_celeba/010248.jpg',\n",
              " 'img_align_celeba/011838.jpg',\n",
              " 'img_align_celeba/010249.jpg',\n",
              " 'img_align_celeba/011840.jpg',\n",
              " 'img_align_celeba/010250.jpg',\n",
              " 'img_align_celeba/011853.jpg',\n",
              " 'img_align_celeba/010251.jpg',\n",
              " 'img_align_celeba/011856.jpg',\n",
              " 'img_align_celeba/010252.jpg',\n",
              " 'img_align_celeba/011862.jpg',\n",
              " 'img_align_celeba/010254.jpg',\n",
              " 'img_align_celeba/011870.jpg',\n",
              " 'img_align_celeba/010255.jpg',\n",
              " 'img_align_celeba/011875.jpg',\n",
              " 'img_align_celeba/010256.jpg',\n",
              " 'img_align_celeba/011879.jpg',\n",
              " 'img_align_celeba/010257.jpg',\n",
              " 'img_align_celeba/011881.jpg',\n",
              " 'img_align_celeba/010258.jpg',\n",
              " 'img_align_celeba/011884.jpg',\n",
              " 'img_align_celeba/010259.jpg',\n",
              " 'img_align_celeba/011891.jpg',\n",
              " 'img_align_celeba/010261.jpg',\n",
              " 'img_align_celeba/011895.jpg',\n",
              " 'img_align_celeba/010263.jpg',\n",
              " 'img_align_celeba/011902.jpg',\n",
              " 'img_align_celeba/010264.jpg',\n",
              " 'img_align_celeba/011906.jpg',\n",
              " 'img_align_celeba/010265.jpg',\n",
              " 'img_align_celeba/011928.jpg',\n",
              " 'img_align_celeba/010267.jpg',\n",
              " 'img_align_celeba/011931.jpg',\n",
              " 'img_align_celeba/010268.jpg',\n",
              " 'img_align_celeba/011946.jpg',\n",
              " 'img_align_celeba/010269.jpg',\n",
              " 'img_align_celeba/011963.jpg',\n",
              " 'img_align_celeba/010270.jpg',\n",
              " 'img_align_celeba/011965.jpg',\n",
              " 'img_align_celeba/010271.jpg',\n",
              " 'img_align_celeba/011968.jpg',\n",
              " 'img_align_celeba/010272.jpg',\n",
              " 'img_align_celeba/011977.jpg',\n",
              " 'img_align_celeba/010273.jpg',\n",
              " 'img_align_celeba/011979.jpg',\n",
              " 'img_align_celeba/010274.jpg',\n",
              " 'img_align_celeba/011984.jpg',\n",
              " 'img_align_celeba/010275.jpg',\n",
              " 'img_align_celeba/011986.jpg',\n",
              " 'img_align_celeba/010276.jpg',\n",
              " 'img_align_celeba/012027.jpg',\n",
              " 'img_align_celeba/010277.jpg',\n",
              " 'img_align_celeba/012047.jpg',\n",
              " 'img_align_celeba/010278.jpg',\n",
              " 'img_align_celeba/012052.jpg',\n",
              " 'img_align_celeba/010280.jpg',\n",
              " 'img_align_celeba/012055.jpg',\n",
              " 'img_align_celeba/010282.jpg',\n",
              " 'img_align_celeba/012072.jpg',\n",
              " 'img_align_celeba/010283.jpg',\n",
              " 'img_align_celeba/012087.jpg',\n",
              " 'img_align_celeba/010284.jpg',\n",
              " 'img_align_celeba/012089.jpg',\n",
              " 'img_align_celeba/010285.jpg',\n",
              " 'img_align_celeba/012090.jpg',\n",
              " 'img_align_celeba/010287.jpg',\n",
              " 'img_align_celeba/012094.jpg',\n",
              " 'img_align_celeba/010288.jpg',\n",
              " 'img_align_celeba/012095.jpg',\n",
              " 'img_align_celeba/010289.jpg',\n",
              " 'img_align_celeba/012124.jpg',\n",
              " 'img_align_celeba/010290.jpg',\n",
              " 'img_align_celeba/012131.jpg',\n",
              " 'img_align_celeba/010291.jpg',\n",
              " 'img_align_celeba/012135.jpg',\n",
              " 'img_align_celeba/010292.jpg',\n",
              " 'img_align_celeba/012177.jpg',\n",
              " 'img_align_celeba/010293.jpg',\n",
              " 'img_align_celeba/012197.jpg',\n",
              " 'img_align_celeba/010294.jpg',\n",
              " 'img_align_celeba/012203.jpg',\n",
              " 'img_align_celeba/010295.jpg',\n",
              " 'img_align_celeba/012209.jpg',\n",
              " 'img_align_celeba/010296.jpg',\n",
              " 'img_align_celeba/012219.jpg',\n",
              " 'img_align_celeba/010297.jpg',\n",
              " 'img_align_celeba/012224.jpg',\n",
              " 'img_align_celeba/010298.jpg',\n",
              " 'img_align_celeba/012228.jpg',\n",
              " 'img_align_celeba/010299.jpg',\n",
              " 'img_align_celeba/012229.jpg',\n",
              " 'img_align_celeba/010300.jpg',\n",
              " 'img_align_celeba/012230.jpg',\n",
              " 'img_align_celeba/010301.jpg',\n",
              " 'img_align_celeba/012232.jpg',\n",
              " 'img_align_celeba/010302.jpg',\n",
              " 'img_align_celeba/012237.jpg',\n",
              " 'img_align_celeba/010303.jpg',\n",
              " 'img_align_celeba/012249.jpg',\n",
              " 'img_align_celeba/010304.jpg',\n",
              " 'img_align_celeba/012258.jpg',\n",
              " 'img_align_celeba/010305.jpg',\n",
              " 'img_align_celeba/012265.jpg',\n",
              " 'img_align_celeba/010306.jpg',\n",
              " 'img_align_celeba/012267.jpg',\n",
              " 'img_align_celeba/010307.jpg',\n",
              " 'img_align_celeba/012275.jpg',\n",
              " 'img_align_celeba/010308.jpg',\n",
              " 'img_align_celeba/012279.jpg',\n",
              " 'img_align_celeba/010310.jpg',\n",
              " 'img_align_celeba/012288.jpg',\n",
              " 'img_align_celeba/010311.jpg',\n",
              " 'img_align_celeba/012289.jpg',\n",
              " 'img_align_celeba/010312.jpg',\n",
              " 'img_align_celeba/012303.jpg',\n",
              " 'img_align_celeba/010313.jpg',\n",
              " 'img_align_celeba/012313.jpg',\n",
              " 'img_align_celeba/010314.jpg',\n",
              " 'img_align_celeba/012316.jpg',\n",
              " 'img_align_celeba/010316.jpg',\n",
              " 'img_align_celeba/012317.jpg',\n",
              " 'img_align_celeba/010317.jpg',\n",
              " 'img_align_celeba/012319.jpg',\n",
              " 'img_align_celeba/010318.jpg',\n",
              " 'img_align_celeba/012332.jpg',\n",
              " 'img_align_celeba/010319.jpg',\n",
              " 'img_align_celeba/012341.jpg',\n",
              " 'img_align_celeba/010320.jpg',\n",
              " 'img_align_celeba/012359.jpg',\n",
              " 'img_align_celeba/010322.jpg',\n",
              " 'img_align_celeba/012371.jpg',\n",
              " 'img_align_celeba/010323.jpg',\n",
              " 'img_align_celeba/012381.jpg',\n",
              " 'img_align_celeba/010324.jpg',\n",
              " 'img_align_celeba/012392.jpg',\n",
              " 'img_align_celeba/010325.jpg',\n",
              " 'img_align_celeba/012394.jpg',\n",
              " 'img_align_celeba/010326.jpg',\n",
              " 'img_align_celeba/012397.jpg',\n",
              " 'img_align_celeba/010327.jpg',\n",
              " 'img_align_celeba/012398.jpg',\n",
              " 'img_align_celeba/010329.jpg',\n",
              " 'img_align_celeba/012399.jpg',\n",
              " 'img_align_celeba/010330.jpg',\n",
              " 'img_align_celeba/012400.jpg',\n",
              " 'img_align_celeba/010331.jpg',\n",
              " 'img_align_celeba/012403.jpg',\n",
              " 'img_align_celeba/010332.jpg',\n",
              " 'img_align_celeba/012405.jpg',\n",
              " 'img_align_celeba/010334.jpg',\n",
              " 'img_align_celeba/012428.jpg',\n",
              " 'img_align_celeba/010335.jpg',\n",
              " 'img_align_celeba/012444.jpg',\n",
              " 'img_align_celeba/010337.jpg',\n",
              " 'img_align_celeba/012475.jpg',\n",
              " 'img_align_celeba/010338.jpg',\n",
              " 'img_align_celeba/012483.jpg',\n",
              " 'img_align_celeba/010339.jpg',\n",
              " 'img_align_celeba/012485.jpg',\n",
              " 'img_align_celeba/010340.jpg',\n",
              " 'img_align_celeba/012492.jpg',\n",
              " 'img_align_celeba/010341.jpg',\n",
              " 'img_align_celeba/012510.jpg',\n",
              " 'img_align_celeba/010342.jpg',\n",
              " 'img_align_celeba/012518.jpg',\n",
              " 'img_align_celeba/010343.jpg',\n",
              " 'img_align_celeba/012519.jpg',\n",
              " 'img_align_celeba/010344.jpg',\n",
              " 'img_align_celeba/012531.jpg',\n",
              " 'img_align_celeba/010345.jpg',\n",
              " 'img_align_celeba/012541.jpg',\n",
              " 'img_align_celeba/010346.jpg',\n",
              " 'img_align_celeba/012547.jpg',\n",
              " 'img_align_celeba/010347.jpg',\n",
              " 'img_align_celeba/012555.jpg',\n",
              " 'img_align_celeba/010348.jpg',\n",
              " 'img_align_celeba/012561.jpg',\n",
              " 'img_align_celeba/010349.jpg',\n",
              " 'img_align_celeba/012577.jpg',\n",
              " 'img_align_celeba/010350.jpg',\n",
              " 'img_align_celeba/012586.jpg',\n",
              " 'img_align_celeba/010351.jpg',\n",
              " 'img_align_celeba/012591.jpg',\n",
              " 'img_align_celeba/010352.jpg',\n",
              " 'img_align_celeba/012598.jpg',\n",
              " 'img_align_celeba/010353.jpg',\n",
              " 'img_align_celeba/012602.jpg',\n",
              " 'img_align_celeba/010354.jpg',\n",
              " 'img_align_celeba/012604.jpg',\n",
              " 'img_align_celeba/010355.jpg',\n",
              " 'img_align_celeba/012611.jpg',\n",
              " 'img_align_celeba/010356.jpg',\n",
              " 'img_align_celeba/012615.jpg',\n",
              " 'img_align_celeba/010357.jpg',\n",
              " 'img_align_celeba/012617.jpg',\n",
              " 'img_align_celeba/010358.jpg',\n",
              " 'img_align_celeba/012623.jpg',\n",
              " 'img_align_celeba/010359.jpg',\n",
              " 'img_align_celeba/012633.jpg',\n",
              " 'img_align_celeba/010360.jpg',\n",
              " 'img_align_celeba/012657.jpg',\n",
              " 'img_align_celeba/010361.jpg',\n",
              " 'img_align_celeba/012679.jpg',\n",
              " 'img_align_celeba/010362.jpg',\n",
              " 'img_align_celeba/012685.jpg',\n",
              " 'img_align_celeba/010363.jpg',\n",
              " 'img_align_celeba/012709.jpg',\n",
              " 'img_align_celeba/010364.jpg',\n",
              " 'img_align_celeba/012712.jpg',\n",
              " 'img_align_celeba/010365.jpg',\n",
              " 'img_align_celeba/012713.jpg',\n",
              " 'img_align_celeba/010366.jpg',\n",
              " 'img_align_celeba/012725.jpg',\n",
              " 'img_align_celeba/010367.jpg',\n",
              " 'img_align_celeba/012747.jpg',\n",
              " 'img_align_celeba/010368.jpg',\n",
              " 'img_align_celeba/012755.jpg',\n",
              " 'img_align_celeba/010369.jpg',\n",
              " 'img_align_celeba/012762.jpg',\n",
              " 'img_align_celeba/010370.jpg',\n",
              " 'img_align_celeba/012765.jpg',\n",
              " 'img_align_celeba/010371.jpg',\n",
              " 'img_align_celeba/012784.jpg',\n",
              " 'img_align_celeba/010372.jpg',\n",
              " 'img_align_celeba/012803.jpg',\n",
              " 'img_align_celeba/010373.jpg',\n",
              " 'img_align_celeba/012807.jpg',\n",
              " 'img_align_celeba/010374.jpg',\n",
              " 'img_align_celeba/012812.jpg',\n",
              " 'img_align_celeba/010376.jpg',\n",
              " 'img_align_celeba/012826.jpg',\n",
              " 'img_align_celeba/010377.jpg',\n",
              " 'img_align_celeba/012840.jpg',\n",
              " 'img_align_celeba/010378.jpg',\n",
              " 'img_align_celeba/012846.jpg',\n",
              " 'img_align_celeba/010379.jpg',\n",
              " 'img_align_celeba/012853.jpg',\n",
              " 'img_align_celeba/010380.jpg',\n",
              " 'img_align_celeba/012866.jpg',\n",
              " 'img_align_celeba/010381.jpg',\n",
              " 'img_align_celeba/012884.jpg',\n",
              " 'img_align_celeba/010382.jpg',\n",
              " 'img_align_celeba/012888.jpg',\n",
              " 'img_align_celeba/010383.jpg',\n",
              " 'img_align_celeba/012890.jpg',\n",
              " 'img_align_celeba/010384.jpg',\n",
              " 'img_align_celeba/012899.jpg',\n",
              " 'img_align_celeba/010385.jpg',\n",
              " 'img_align_celeba/012911.jpg',\n",
              " 'img_align_celeba/010386.jpg',\n",
              " 'img_align_celeba/012916.jpg',\n",
              " 'img_align_celeba/010387.jpg',\n",
              " 'img_align_celeba/012927.jpg',\n",
              " 'img_align_celeba/010388.jpg',\n",
              " 'img_align_celeba/012933.jpg',\n",
              " 'img_align_celeba/010389.jpg',\n",
              " 'img_align_celeba/012938.jpg',\n",
              " 'img_align_celeba/010391.jpg',\n",
              " 'img_align_celeba/012949.jpg',\n",
              " 'img_align_celeba/010392.jpg',\n",
              " 'img_align_celeba/012957.jpg',\n",
              " 'img_align_celeba/010393.jpg',\n",
              " 'img_align_celeba/012966.jpg',\n",
              " 'img_align_celeba/010394.jpg',\n",
              " 'img_align_celeba/012968.jpg',\n",
              " 'img_align_celeba/010395.jpg',\n",
              " 'img_align_celeba/012975.jpg',\n",
              " 'img_align_celeba/010397.jpg',\n",
              " 'img_align_celeba/012976.jpg',\n",
              " 'img_align_celeba/010398.jpg',\n",
              " 'img_align_celeba/012984.jpg',\n",
              " 'img_align_celeba/010399.jpg',\n",
              " 'img_align_celeba/012995.jpg',\n",
              " 'img_align_celeba/010400.jpg',\n",
              " 'img_align_celeba/012998.jpg',\n",
              " 'img_align_celeba/010401.jpg',\n",
              " 'img_align_celeba/012999.jpg',\n",
              " 'img_align_celeba/010402.jpg',\n",
              " 'img_align_celeba/013005.jpg',\n",
              " 'img_align_celeba/010403.jpg',\n",
              " 'img_align_celeba/013007.jpg',\n",
              " 'img_align_celeba/010404.jpg',\n",
              " 'img_align_celeba/013009.jpg',\n",
              " 'img_align_celeba/010405.jpg',\n",
              " 'img_align_celeba/013011.jpg',\n",
              " 'img_align_celeba/010406.jpg',\n",
              " 'img_align_celeba/013014.jpg',\n",
              " 'img_align_celeba/010407.jpg',\n",
              " 'img_align_celeba/013025.jpg',\n",
              " 'img_align_celeba/010408.jpg',\n",
              " 'img_align_celeba/013033.jpg',\n",
              " 'img_align_celeba/010409.jpg',\n",
              " 'img_align_celeba/013039.jpg',\n",
              " 'img_align_celeba/010410.jpg',\n",
              " 'img_align_celeba/013044.jpg',\n",
              " 'img_align_celeba/010412.jpg',\n",
              " 'img_align_celeba/013055.jpg',\n",
              " 'img_align_celeba/010413.jpg',\n",
              " 'img_align_celeba/013058.jpg',\n",
              " 'img_align_celeba/010414.jpg',\n",
              " 'img_align_celeba/013062.jpg',\n",
              " 'img_align_celeba/010415.jpg',\n",
              " 'img_align_celeba/013071.jpg',\n",
              " 'img_align_celeba/010416.jpg',\n",
              " 'img_align_celeba/013072.jpg',\n",
              " 'img_align_celeba/010417.jpg',\n",
              " 'img_align_celeba/013077.jpg',\n",
              " 'img_align_celeba/010419.jpg',\n",
              " 'img_align_celeba/013080.jpg',\n",
              " 'img_align_celeba/010420.jpg',\n",
              " 'img_align_celeba/013091.jpg',\n",
              " 'img_align_celeba/010421.jpg',\n",
              " 'img_align_celeba/013094.jpg',\n",
              " 'img_align_celeba/010422.jpg',\n",
              " 'img_align_celeba/013095.jpg',\n",
              " 'img_align_celeba/010423.jpg',\n",
              " 'img_align_celeba/013096.jpg',\n",
              " 'img_align_celeba/010424.jpg',\n",
              " 'img_align_celeba/013098.jpg',\n",
              " 'img_align_celeba/010425.jpg',\n",
              " 'img_align_celeba/013102.jpg',\n",
              " 'img_align_celeba/010426.jpg',\n",
              " 'img_align_celeba/013106.jpg',\n",
              " 'img_align_celeba/010427.jpg',\n",
              " 'img_align_celeba/013124.jpg',\n",
              " 'img_align_celeba/010428.jpg',\n",
              " 'img_align_celeba/013126.jpg',\n",
              " 'img_align_celeba/010429.jpg',\n",
              " 'img_align_celeba/013130.jpg',\n",
              " 'img_align_celeba/010430.jpg',\n",
              " 'img_align_celeba/013134.jpg',\n",
              " 'img_align_celeba/010431.jpg',\n",
              " 'img_align_celeba/013136.jpg',\n",
              " 'img_align_celeba/010432.jpg',\n",
              " 'img_align_celeba/013137.jpg',\n",
              " 'img_align_celeba/010433.jpg',\n",
              " 'img_align_celeba/013139.jpg',\n",
              " 'img_align_celeba/010434.jpg',\n",
              " 'img_align_celeba/013159.jpg',\n",
              " 'img_align_celeba/010435.jpg',\n",
              " 'img_align_celeba/013164.jpg',\n",
              " 'img_align_celeba/010436.jpg',\n",
              " 'img_align_celeba/013185.jpg',\n",
              " 'img_align_celeba/010439.jpg',\n",
              " 'img_align_celeba/013198.jpg',\n",
              " 'img_align_celeba/010440.jpg',\n",
              " 'img_align_celeba/013204.jpg',\n",
              " 'img_align_celeba/010441.jpg',\n",
              " 'img_align_celeba/013210.jpg',\n",
              " 'img_align_celeba/010442.jpg',\n",
              " 'img_align_celeba/013224.jpg',\n",
              " 'img_align_celeba/010443.jpg',\n",
              " 'img_align_celeba/013227.jpg',\n",
              " 'img_align_celeba/010444.jpg',\n",
              " 'img_align_celeba/013249.jpg',\n",
              " 'img_align_celeba/010445.jpg',\n",
              " 'img_align_celeba/013251.jpg',\n",
              " 'img_align_celeba/010447.jpg',\n",
              " 'img_align_celeba/013252.jpg',\n",
              " 'img_align_celeba/010448.jpg',\n",
              " 'img_align_celeba/013254.jpg',\n",
              " 'img_align_celeba/010450.jpg',\n",
              " 'img_align_celeba/013258.jpg',\n",
              " 'img_align_celeba/010451.jpg',\n",
              " 'img_align_celeba/013267.jpg',\n",
              " 'img_align_celeba/010452.jpg',\n",
              " 'img_align_celeba/013276.jpg',\n",
              " 'img_align_celeba/010453.jpg',\n",
              " 'img_align_celeba/013282.jpg',\n",
              " 'img_align_celeba/010454.jpg',\n",
              " 'img_align_celeba/013285.jpg',\n",
              " 'img_align_celeba/010455.jpg',\n",
              " 'img_align_celeba/013300.jpg',\n",
              " 'img_align_celeba/010456.jpg',\n",
              " 'img_align_celeba/013302.jpg',\n",
              " 'img_align_celeba/010457.jpg',\n",
              " 'img_align_celeba/013305.jpg',\n",
              " 'img_align_celeba/010459.jpg',\n",
              " 'img_align_celeba/013306.jpg',\n",
              " 'img_align_celeba/010460.jpg',\n",
              " 'img_align_celeba/013319.jpg',\n",
              " 'img_align_celeba/010461.jpg',\n",
              " 'img_align_celeba/013329.jpg',\n",
              " 'img_align_celeba/010462.jpg',\n",
              " 'img_align_celeba/013333.jpg',\n",
              " 'img_align_celeba/010463.jpg',\n",
              " 'img_align_celeba/013340.jpg',\n",
              " 'img_align_celeba/010464.jpg',\n",
              " 'img_align_celeba/013341.jpg',\n",
              " 'img_align_celeba/010465.jpg',\n",
              " 'img_align_celeba/013353.jpg',\n",
              " 'img_align_celeba/010466.jpg',\n",
              " 'img_align_celeba/013354.jpg',\n",
              " 'img_align_celeba/010467.jpg',\n",
              " 'img_align_celeba/013394.jpg',\n",
              " 'img_align_celeba/010468.jpg',\n",
              " 'img_align_celeba/013396.jpg',\n",
              " 'img_align_celeba/010469.jpg',\n",
              " 'img_align_celeba/013401.jpg',\n",
              " 'img_align_celeba/010470.jpg',\n",
              " 'img_align_celeba/013404.jpg',\n",
              " 'img_align_celeba/010471.jpg',\n",
              " 'img_align_celeba/013405.jpg',\n",
              " 'img_align_celeba/010472.jpg',\n",
              " 'img_align_celeba/013428.jpg',\n",
              " 'img_align_celeba/010473.jpg',\n",
              " 'img_align_celeba/013451.jpg',\n",
              " 'img_align_celeba/010474.jpg',\n",
              " 'img_align_celeba/013466.jpg',\n",
              " 'img_align_celeba/010475.jpg',\n",
              " 'img_align_celeba/013473.jpg',\n",
              " 'img_align_celeba/010477.jpg',\n",
              " 'img_align_celeba/013474.jpg',\n",
              " 'img_align_celeba/010478.jpg',\n",
              " 'img_align_celeba/013477.jpg',\n",
              " 'img_align_celeba/010479.jpg',\n",
              " 'img_align_celeba/013487.jpg',\n",
              " 'img_align_celeba/010480.jpg',\n",
              " 'img_align_celeba/013494.jpg',\n",
              " 'img_align_celeba/010481.jpg',\n",
              " 'img_align_celeba/013498.jpg',\n",
              " 'img_align_celeba/010482.jpg',\n",
              " 'img_align_celeba/013500.jpg',\n",
              " 'img_align_celeba/010483.jpg',\n",
              " 'img_align_celeba/013503.jpg',\n",
              " 'img_align_celeba/010484.jpg',\n",
              " 'img_align_celeba/013506.jpg',\n",
              " 'img_align_celeba/010486.jpg',\n",
              " 'img_align_celeba/013533.jpg',\n",
              " 'img_align_celeba/010487.jpg',\n",
              " 'img_align_celeba/013537.jpg',\n",
              " 'img_align_celeba/010488.jpg',\n",
              " 'img_align_celeba/013546.jpg',\n",
              " 'img_align_celeba/010489.jpg',\n",
              " 'img_align_celeba/013553.jpg',\n",
              " 'img_align_celeba/010492.jpg',\n",
              " 'img_align_celeba/013568.jpg',\n",
              " 'img_align_celeba/010494.jpg',\n",
              " 'img_align_celeba/013569.jpg',\n",
              " 'img_align_celeba/010495.jpg',\n",
              " 'img_align_celeba/013579.jpg',\n",
              " 'img_align_celeba/010496.jpg',\n",
              " 'img_align_celeba/013582.jpg',\n",
              " 'img_align_celeba/010497.jpg',\n",
              " 'img_align_celeba/013595.jpg',\n",
              " 'img_align_celeba/010498.jpg',\n",
              " 'img_align_celeba/013606.jpg',\n",
              " 'img_align_celeba/010499.jpg',\n",
              " 'img_align_celeba/013608.jpg',\n",
              " 'img_align_celeba/010500.jpg',\n",
              " 'img_align_celeba/013611.jpg',\n",
              " 'img_align_celeba/010501.jpg',\n",
              " 'img_align_celeba/013627.jpg',\n",
              " 'img_align_celeba/010502.jpg',\n",
              " 'img_align_celeba/013629.jpg',\n",
              " 'img_align_celeba/010503.jpg',\n",
              " 'img_align_celeba/013631.jpg',\n",
              " 'img_align_celeba/010504.jpg',\n",
              " 'img_align_celeba/013633.jpg',\n",
              " 'img_align_celeba/010507.jpg',\n",
              " 'img_align_celeba/013643.jpg',\n",
              " 'img_align_celeba/010509.jpg',\n",
              " 'img_align_celeba/013646.jpg',\n",
              " 'img_align_celeba/010511.jpg',\n",
              " 'img_align_celeba/013649.jpg',\n",
              " 'img_align_celeba/010512.jpg',\n",
              " 'img_align_celeba/013683.jpg',\n",
              " 'img_align_celeba/010513.jpg',\n",
              " 'img_align_celeba/013697.jpg',\n",
              " 'img_align_celeba/010514.jpg',\n",
              " 'img_align_celeba/013700.jpg',\n",
              " 'img_align_celeba/010515.jpg',\n",
              " 'img_align_celeba/013706.jpg',\n",
              " 'img_align_celeba/010516.jpg',\n",
              " 'img_align_celeba/013709.jpg',\n",
              " 'img_align_celeba/010517.jpg',\n",
              " 'img_align_celeba/013710.jpg',\n",
              " 'img_align_celeba/010519.jpg',\n",
              " 'img_align_celeba/013711.jpg',\n",
              " 'img_align_celeba/010520.jpg',\n",
              " 'img_align_celeba/013760.jpg',\n",
              " 'img_align_celeba/010521.jpg',\n",
              " 'img_align_celeba/013771.jpg',\n",
              " 'img_align_celeba/010522.jpg',\n",
              " 'img_align_celeba/013773.jpg',\n",
              " 'img_align_celeba/010523.jpg',\n",
              " 'img_align_celeba/013780.jpg',\n",
              " 'img_align_celeba/010524.jpg',\n",
              " 'img_align_celeba/013787.jpg',\n",
              " 'img_align_celeba/010525.jpg',\n",
              " 'img_align_celeba/013796.jpg',\n",
              " 'img_align_celeba/010526.jpg',\n",
              " 'img_align_celeba/013808.jpg',\n",
              " 'img_align_celeba/010527.jpg',\n",
              " 'img_align_celeba/013824.jpg',\n",
              " 'img_align_celeba/010528.jpg',\n",
              " 'img_align_celeba/013825.jpg',\n",
              " 'img_align_celeba/010529.jpg',\n",
              " 'img_align_celeba/013849.jpg',\n",
              " 'img_align_celeba/010530.jpg',\n",
              " 'img_align_celeba/013851.jpg',\n",
              " 'img_align_celeba/010531.jpg',\n",
              " 'img_align_celeba/013853.jpg',\n",
              " 'img_align_celeba/010532.jpg',\n",
              " 'img_align_celeba/013862.jpg',\n",
              " 'img_align_celeba/010533.jpg',\n",
              " 'img_align_celeba/013874.jpg',\n",
              " 'img_align_celeba/010534.jpg',\n",
              " 'img_align_celeba/013879.jpg',\n",
              " 'img_align_celeba/010535.jpg',\n",
              " 'img_align_celeba/013881.jpg',\n",
              " 'img_align_celeba/010536.jpg',\n",
              " 'img_align_celeba/013883.jpg',\n",
              " 'img_align_celeba/010538.jpg',\n",
              " 'img_align_celeba/013892.jpg',\n",
              " 'img_align_celeba/010539.jpg',\n",
              " 'img_align_celeba/013897.jpg',\n",
              " 'img_align_celeba/010540.jpg',\n",
              " 'img_align_celeba/013899.jpg',\n",
              " 'img_align_celeba/010541.jpg',\n",
              " 'img_align_celeba/013901.jpg',\n",
              " 'img_align_celeba/010542.jpg',\n",
              " 'img_align_celeba/013908.jpg',\n",
              " 'img_align_celeba/010543.jpg',\n",
              " 'img_align_celeba/013930.jpg',\n",
              " 'img_align_celeba/010544.jpg',\n",
              " 'img_align_celeba/013944.jpg',\n",
              " 'img_align_celeba/010545.jpg',\n",
              " 'img_align_celeba/013954.jpg',\n",
              " 'img_align_celeba/010546.jpg',\n",
              " 'img_align_celeba/013956.jpg',\n",
              " 'img_align_celeba/010547.jpg',\n",
              " 'img_align_celeba/013961.jpg',\n",
              " 'img_align_celeba/010548.jpg',\n",
              " 'img_align_celeba/013982.jpg',\n",
              " 'img_align_celeba/010551.jpg',\n",
              " 'img_align_celeba/014007.jpg',\n",
              " 'img_align_celeba/010552.jpg',\n",
              " 'img_align_celeba/014018.jpg',\n",
              " 'img_align_celeba/010553.jpg',\n",
              " 'img_align_celeba/014045.jpg',\n",
              " 'img_align_celeba/010554.jpg',\n",
              " 'img_align_celeba/014057.jpg',\n",
              " 'img_align_celeba/010555.jpg',\n",
              " 'img_align_celeba/014063.jpg',\n",
              " 'img_align_celeba/010556.jpg',\n",
              " 'img_align_celeba/014086.jpg',\n",
              " 'img_align_celeba/010557.jpg',\n",
              " 'img_align_celeba/014099.jpg',\n",
              " 'img_align_celeba/010558.jpg',\n",
              " 'img_align_celeba/014109.jpg',\n",
              " 'img_align_celeba/010559.jpg',\n",
              " 'img_align_celeba/014115.jpg',\n",
              " 'img_align_celeba/010560.jpg',\n",
              " 'img_align_celeba/014130.jpg',\n",
              " 'img_align_celeba/010561.jpg',\n",
              " 'img_align_celeba/014139.jpg',\n",
              " 'img_align_celeba/010562.jpg',\n",
              " 'img_align_celeba/014164.jpg',\n",
              " 'img_align_celeba/010563.jpg',\n",
              " 'img_align_celeba/014177.jpg',\n",
              " 'img_align_celeba/010564.jpg',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "OzlXoRsJEppm",
        "colab_type": "code",
        "outputId": "db85dbea-21fd-4c0a-a068-d3c2e69bdf65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17812
        }
      },
      "cell_type": "code",
      "source": [
        "train_labels[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "cutaEpyHZLvW",
        "colab_type": "code",
        "outputId": "7c402ad2-4bc2-4cbb-f391-43444ca1a155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', \n",
        "                        input_shape=(128, 128, 3)))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 32)      416       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               16384500  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 1002      \n",
            "=================================================================\n",
            "Total params: 16,427,070\n",
            "Trainable params: 16,427,070\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f4F-Mx-WmxF4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JqYxpquQq1nG",
        "colab_type": "code",
        "outputId": "337ac809-9ca9-4ed3-be43-ccbe39589763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "DwnI8h3Sal0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy import misc\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.utils import np_utils\n",
        "\n",
        "def prepare_train(file_paths, labels):\n",
        "  images = [misc.imread(path) for path in file_paths]\n",
        "  new_img=[]\n",
        "  for i in range(len(images)):\n",
        "    img=cv2.resize(images[i],(128,128))\n",
        "    new_img.append(img)\n",
        "    \n",
        "     \n",
        "  images=np.asarray(new_img)\n",
        "  images=images/255\n",
        "  \n",
        "  labels=np.asarray(labels)\n",
        "  labels=np_utils.to_categorical(labels,2)\n",
        "  \n",
        "  return images,labels\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GGCQa8FRgFdi",
        "colab_type": "code",
        "outputId": "3902388b-6d2e-447a-ccdb-a59602768b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23270
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "for i in range(20,40):\n",
        "  X_train,y_train=prepare_train(train_files[i],train_labels[i])\n",
        "  print(\"Attribute: \",i)\n",
        "  checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
        "                               save_best_only=True)\n",
        "  stop=EarlyStopping(monitor='val_loss',\n",
        "                              min_delta=0,\n",
        "                              patience=10,\n",
        "                              verbose=1, mode='auto')\n",
        "  hist = model.fit(X_train, y_train, batch_size=200, epochs=50,\n",
        "          validation_split=0.2, callbacks=[checkpointer,stop], \n",
        "          verbose=2, shuffle=True) \n",
        "  model.load_weights('model.weights.best.hdf5')\n",
        "  model.save('model'+str(i)+'.h5')\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attribute:  20\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 18s - loss: 0.8092 - acc: 0.5783 - val_loss: 0.5436 - val_acc: 0.7960\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54359, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.4653 - acc: 0.7835 - val_loss: 0.2875 - val_acc: 0.8905\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54359 to 0.28748, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 12s - loss: 0.3499 - acc: 0.8495 - val_loss: 0.3217 - val_acc: 0.8650\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.28748\n",
            "Epoch 4/50\n",
            " - 12s - loss: 0.2620 - acc: 0.8951 - val_loss: 0.1819 - val_acc: 0.9285\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.28748 to 0.18194, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 12s - loss: 0.1969 - acc: 0.9221 - val_loss: 0.1328 - val_acc: 0.9455\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.18194 to 0.13284, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 12s - loss: 0.1646 - acc: 0.9360 - val_loss: 0.2290 - val_acc: 0.9075\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.13284\n",
            "Epoch 7/50\n",
            " - 12s - loss: 0.1378 - acc: 0.9460 - val_loss: 0.2534 - val_acc: 0.9070\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.13284\n",
            "Epoch 8/50\n",
            " - 12s - loss: 0.1294 - acc: 0.9496 - val_loss: 0.1105 - val_acc: 0.9605\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.13284 to 0.11054, saving model to model.weights.best.hdf5\n",
            "Epoch 9/50\n",
            " - 12s - loss: 0.1051 - acc: 0.9592 - val_loss: 0.1711 - val_acc: 0.9320\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.11054\n",
            "Epoch 10/50\n",
            " - 12s - loss: 0.0855 - acc: 0.9696 - val_loss: 0.2119 - val_acc: 0.9220\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.11054\n",
            "Epoch 11/50\n",
            " - 12s - loss: 0.0784 - acc: 0.9705 - val_loss: 0.1605 - val_acc: 0.9395\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.11054\n",
            "Epoch 12/50\n",
            " - 12s - loss: 0.0622 - acc: 0.9769 - val_loss: 0.2140 - val_acc: 0.9280\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.11054\n",
            "Epoch 13/50\n",
            " - 12s - loss: 0.0619 - acc: 0.9775 - val_loss: 0.1721 - val_acc: 0.9390\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.11054\n",
            "Epoch 14/50\n",
            " - 12s - loss: 0.0489 - acc: 0.9831 - val_loss: 0.1464 - val_acc: 0.9525\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.11054\n",
            "Epoch 15/50\n",
            " - 12s - loss: 0.0467 - acc: 0.9826 - val_loss: 0.1760 - val_acc: 0.9490\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.11054\n",
            "Epoch 16/50\n",
            " - 12s - loss: 0.0421 - acc: 0.9850 - val_loss: 0.1638 - val_acc: 0.9500\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.11054\n",
            "Epoch 17/50\n",
            " - 11s - loss: 0.0343 - acc: 0.9895 - val_loss: 0.2006 - val_acc: 0.9405\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.11054\n",
            "Epoch 18/50\n",
            " - 11s - loss: 0.0315 - acc: 0.9888 - val_loss: 0.2740 - val_acc: 0.9240\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.11054\n",
            "Epoch 00018: early stopping\n",
            "Attribute:  21\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.7302 - acc: 0.6128 - val_loss: 0.5587 - val_acc: 0.7155\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.55872, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.4333 - acc: 0.8031 - val_loss: 0.3668 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.55872 to 0.36677, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.3531 - acc: 0.8500 - val_loss: 0.3226 - val_acc: 0.8760\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.36677 to 0.32263, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 12s - loss: 0.3114 - acc: 0.8707 - val_loss: 0.2913 - val_acc: 0.8890\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.32263 to 0.29133, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 12s - loss: 0.2861 - acc: 0.8849 - val_loss: 0.2873 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.29133 to 0.28725, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.2675 - acc: 0.8951 - val_loss: 0.2630 - val_acc: 0.9005\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.28725 to 0.26298, saving model to model.weights.best.hdf5\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.2411 - acc: 0.9050 - val_loss: 0.2538 - val_acc: 0.9040\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.26298 to 0.25383, saving model to model.weights.best.hdf5\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.2255 - acc: 0.9125 - val_loss: 0.2504 - val_acc: 0.9050\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.25383 to 0.25038, saving model to model.weights.best.hdf5\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.2137 - acc: 0.9167 - val_loss: 0.2340 - val_acc: 0.9155\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.25038 to 0.23395, saving model to model.weights.best.hdf5\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.1948 - acc: 0.9236 - val_loss: 0.2392 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.23395\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.1786 - acc: 0.9321 - val_loss: 0.2443 - val_acc: 0.9065\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.23395\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.1664 - acc: 0.9364 - val_loss: 0.2405 - val_acc: 0.9125\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.23395\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.1589 - acc: 0.9406 - val_loss: 0.2442 - val_acc: 0.9095\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.23395\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.1471 - acc: 0.9450 - val_loss: 0.2419 - val_acc: 0.9120\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.23395\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.1340 - acc: 0.9475 - val_loss: 0.2659 - val_acc: 0.9040\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.23395\n",
            "Epoch 16/50\n",
            " - 11s - loss: 0.1208 - acc: 0.9546 - val_loss: 0.2492 - val_acc: 0.9125\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.23395\n",
            "Epoch 17/50\n",
            " - 11s - loss: 0.1164 - acc: 0.9535 - val_loss: 0.2744 - val_acc: 0.9090\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.23395\n",
            "Epoch 18/50\n",
            " - 11s - loss: 0.1089 - acc: 0.9575 - val_loss: 0.2544 - val_acc: 0.9135\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.23395\n",
            "Epoch 19/50\n",
            " - 11s - loss: 0.1005 - acc: 0.9625 - val_loss: 0.2576 - val_acc: 0.9185\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.23395\n",
            "Epoch 00019: early stopping\n",
            "Attribute:  22\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.7204 - acc: 0.6053 - val_loss: 0.6627 - val_acc: 0.6320\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.66270, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.5240 - acc: 0.7408 - val_loss: 0.5208 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.66270 to 0.52083, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.4356 - acc: 0.7972 - val_loss: 0.4250 - val_acc: 0.7950\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.52083 to 0.42502, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.3777 - acc: 0.8347 - val_loss: 0.3931 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.42502 to 0.39310, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.3334 - acc: 0.8473 - val_loss: 0.3468 - val_acc: 0.8345\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.39310 to 0.34683, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.3074 - acc: 0.8666 - val_loss: 0.4128 - val_acc: 0.8155\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.34683\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.2925 - acc: 0.8751 - val_loss: 0.2823 - val_acc: 0.8800\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.34683 to 0.28226, saving model to model.weights.best.hdf5\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.2764 - acc: 0.8841 - val_loss: 0.2675 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.28226 to 0.26754, saving model to model.weights.best.hdf5\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.2522 - acc: 0.8954 - val_loss: 0.3099 - val_acc: 0.8720\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.26754\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.2346 - acc: 0.9035 - val_loss: 0.2861 - val_acc: 0.8810\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.26754\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.2148 - acc: 0.9121 - val_loss: 0.4174 - val_acc: 0.8250\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.26754\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.2002 - acc: 0.9198 - val_loss: 0.3646 - val_acc: 0.8545\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.26754\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.1907 - acc: 0.9263 - val_loss: 0.3762 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.26754\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.1781 - acc: 0.9294 - val_loss: 0.3968 - val_acc: 0.8475\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.26754\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.1601 - acc: 0.9362 - val_loss: 0.2919 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.26754\n",
            "Epoch 16/50\n",
            " - 11s - loss: 0.1519 - acc: 0.9414 - val_loss: 0.4316 - val_acc: 0.8420\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.26754\n",
            "Epoch 17/50\n",
            " - 11s - loss: 0.1391 - acc: 0.9452 - val_loss: 0.3629 - val_acc: 0.8730\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.26754\n",
            "Epoch 18/50\n",
            " - 12s - loss: 0.1321 - acc: 0.9492 - val_loss: 0.3719 - val_acc: 0.8690\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.26754\n",
            "Epoch 00018: early stopping\n",
            "Attribute:  23\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.7517 - acc: 0.5059 - val_loss: 0.6971 - val_acc: 0.4530\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69707, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.6898 - acc: 0.5266 - val_loss: 0.7023 - val_acc: 0.4475\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69707\n",
            "Epoch 3/50\n",
            " - 12s - loss: 0.6791 - acc: 0.5618 - val_loss: 0.6733 - val_acc: 0.5855\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69707 to 0.67333, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.6349 - acc: 0.6341 - val_loss: 0.5912 - val_acc: 0.6935\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.67333 to 0.59123, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 12s - loss: 0.5870 - acc: 0.6935 - val_loss: 0.5635 - val_acc: 0.7070\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.59123 to 0.56346, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 12s - loss: 0.5451 - acc: 0.7246 - val_loss: 0.5702 - val_acc: 0.6995\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.56346\n",
            "Epoch 7/50\n",
            " - 12s - loss: 0.5218 - acc: 0.7405 - val_loss: 0.5919 - val_acc: 0.6985\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.56346\n",
            "Epoch 8/50\n",
            " - 12s - loss: 0.4937 - acc: 0.7596 - val_loss: 0.5236 - val_acc: 0.7355\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.56346 to 0.52361, saving model to model.weights.best.hdf5\n",
            "Epoch 9/50\n",
            " - 12s - loss: 0.4485 - acc: 0.7881 - val_loss: 0.5663 - val_acc: 0.7175\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.52361\n",
            "Epoch 10/50\n",
            " - 12s - loss: 0.3947 - acc: 0.8205 - val_loss: 0.5853 - val_acc: 0.7055\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.52361\n",
            "Epoch 11/50\n",
            " - 12s - loss: 0.3480 - acc: 0.8450 - val_loss: 0.6410 - val_acc: 0.6920\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.52361\n",
            "Epoch 12/50\n",
            " - 12s - loss: 0.2897 - acc: 0.8777 - val_loss: 0.6409 - val_acc: 0.7035\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.52361\n",
            "Epoch 13/50\n",
            " - 12s - loss: 0.2394 - acc: 0.9019 - val_loss: 0.7958 - val_acc: 0.6735\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.52361\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.1977 - acc: 0.9216 - val_loss: 0.8487 - val_acc: 0.6810\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.52361\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.1624 - acc: 0.9376 - val_loss: 0.8169 - val_acc: 0.7025\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.52361\n",
            "Epoch 16/50\n",
            " - 11s - loss: 0.1372 - acc: 0.9511 - val_loss: 0.9768 - val_acc: 0.6730\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.52361\n",
            "Epoch 17/50\n",
            " - 11s - loss: 0.1093 - acc: 0.9606 - val_loss: 0.9815 - val_acc: 0.6815\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.52361\n",
            "Epoch 18/50\n",
            " - 11s - loss: 0.0931 - acc: 0.9665 - val_loss: 0.9810 - val_acc: 0.6950\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.52361\n",
            "Epoch 00018: early stopping\n",
            "Attribute:  24\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.6935 - acc: 0.6120 - val_loss: 0.5951 - val_acc: 0.6890\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59505, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.5237 - acc: 0.7404 - val_loss: 0.4276 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.59505 to 0.42764, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.4284 - acc: 0.7999 - val_loss: 0.3583 - val_acc: 0.8440\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.42764 to 0.35827, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.3752 - acc: 0.8289 - val_loss: 0.3367 - val_acc: 0.8515\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.35827 to 0.33671, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.3382 - acc: 0.8421 - val_loss: 0.3367 - val_acc: 0.8415\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.33671 to 0.33667, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 12s - loss: 0.3141 - acc: 0.8614 - val_loss: 0.2926 - val_acc: 0.8735\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.33667 to 0.29259, saving model to model.weights.best.hdf5\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.2972 - acc: 0.8679 - val_loss: 0.2925 - val_acc: 0.8690\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.29259 to 0.29247, saving model to model.weights.best.hdf5\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.2766 - acc: 0.8776 - val_loss: 0.2835 - val_acc: 0.8755\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.29247 to 0.28352, saving model to model.weights.best.hdf5\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.2651 - acc: 0.8841 - val_loss: 0.3247 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.28352\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.2482 - acc: 0.8955 - val_loss: 0.2833 - val_acc: 0.8765\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.28352 to 0.28334, saving model to model.weights.best.hdf5\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.2267 - acc: 0.9042 - val_loss: 0.2900 - val_acc: 0.8775\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.28334\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.2069 - acc: 0.9156 - val_loss: 0.2795 - val_acc: 0.8815\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.28334 to 0.27955, saving model to model.weights.best.hdf5\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.1935 - acc: 0.9185 - val_loss: 0.2757 - val_acc: 0.8825\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.27955 to 0.27566, saving model to model.weights.best.hdf5\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.1747 - acc: 0.9261 - val_loss: 0.3085 - val_acc: 0.8695\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.27566\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.1582 - acc: 0.9364 - val_loss: 0.2833 - val_acc: 0.8780\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.27566\n",
            "Epoch 16/50\n",
            " - 11s - loss: 0.1486 - acc: 0.9426 - val_loss: 0.2873 - val_acc: 0.8745\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.27566\n",
            "Epoch 17/50\n",
            " - 11s - loss: 0.1182 - acc: 0.9540 - val_loss: 0.3215 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.27566\n",
            "Epoch 18/50\n",
            " - 11s - loss: 0.1121 - acc: 0.9566 - val_loss: 0.3307 - val_acc: 0.8725\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.27566\n",
            "Epoch 19/50\n",
            " - 11s - loss: 0.1017 - acc: 0.9642 - val_loss: 0.3721 - val_acc: 0.8585\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.27566\n",
            "Epoch 20/50\n",
            " - 11s - loss: 0.0903 - acc: 0.9666 - val_loss: 0.3614 - val_acc: 0.8770\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.27566\n",
            "Epoch 21/50\n",
            " - 11s - loss: 0.0753 - acc: 0.9735 - val_loss: 0.3541 - val_acc: 0.8665\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.27566\n",
            "Epoch 22/50\n",
            " - 11s - loss: 0.0696 - acc: 0.9745 - val_loss: 0.3879 - val_acc: 0.8605\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.27566\n",
            "Epoch 23/50\n",
            " - 11s - loss: 0.0555 - acc: 0.9820 - val_loss: 0.4088 - val_acc: 0.8705\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.27566\n",
            "Epoch 00023: early stopping\n",
            "Attribute:  25\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.7738 - acc: 0.5270 - val_loss: 0.6866 - val_acc: 0.5540\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68657, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.6794 - acc: 0.5666 - val_loss: 0.6809 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68657 to 0.68093, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.6669 - acc: 0.5976 - val_loss: 0.6799 - val_acc: 0.5755\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68093 to 0.67985, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.6480 - acc: 0.6234 - val_loss: 0.6772 - val_acc: 0.5885\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.67985 to 0.67724, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.6246 - acc: 0.6378 - val_loss: 0.6874 - val_acc: 0.5820\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.67724\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.5964 - acc: 0.6706 - val_loss: 0.6757 - val_acc: 0.6030\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.67724 to 0.67567, saving model to model.weights.best.hdf5\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.5469 - acc: 0.7184 - val_loss: 0.7033 - val_acc: 0.6105\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.67567\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.4929 - acc: 0.7565 - val_loss: 0.7156 - val_acc: 0.6050\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.67567\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.4236 - acc: 0.8064 - val_loss: 0.7949 - val_acc: 0.5915\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.67567\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.3396 - acc: 0.8482 - val_loss: 0.8729 - val_acc: 0.5900\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.67567\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.2750 - acc: 0.8840 - val_loss: 0.9132 - val_acc: 0.5730\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.67567\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.2038 - acc: 0.9176 - val_loss: 1.0480 - val_acc: 0.5840\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.67567\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.1497 - acc: 0.9450 - val_loss: 1.1875 - val_acc: 0.5845\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.67567\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.1325 - acc: 0.9519 - val_loss: 1.3267 - val_acc: 0.5680\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.67567\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.0940 - acc: 0.9691 - val_loss: 1.4012 - val_acc: 0.5745\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.67567\n",
            "Epoch 16/50\n",
            " - 11s - loss: 0.0759 - acc: 0.9755 - val_loss: 1.4298 - val_acc: 0.5795\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.67567\n",
            "Epoch 00016: early stopping\n",
            "Attribute:  26\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.6104 - acc: 0.6571 - val_loss: 0.2234 - val_acc: 0.9090\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.22337, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.3501 - acc: 0.8572 - val_loss: 0.2552 - val_acc: 0.8865\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.22337\n",
            "Epoch 3/50\n",
            " - 12s - loss: 0.2903 - acc: 0.8810 - val_loss: 0.2185 - val_acc: 0.9045\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.22337 to 0.21847, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 12s - loss: 0.2710 - acc: 0.8906 - val_loss: 0.2603 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.21847\n",
            "Epoch 5/50\n",
            " - 12s - loss: 0.2523 - acc: 0.8956 - val_loss: 0.3366 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.21847\n",
            "Epoch 6/50\n",
            " - 12s - loss: 0.2230 - acc: 0.9087 - val_loss: 0.2540 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.21847\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.1869 - acc: 0.9274 - val_loss: 0.3674 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.21847\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.1498 - acc: 0.9411 - val_loss: 0.3239 - val_acc: 0.8720\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.21847\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.1280 - acc: 0.9512 - val_loss: 0.3209 - val_acc: 0.8775\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.21847\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.1088 - acc: 0.9561 - val_loss: 0.3842 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.21847\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0801 - acc: 0.9714 - val_loss: 0.3684 - val_acc: 0.8790\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.21847\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0618 - acc: 0.9793 - val_loss: 0.4478 - val_acc: 0.8730\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.21847\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0467 - acc: 0.9854 - val_loss: 0.4806 - val_acc: 0.8665\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.21847\n",
            "Epoch 00013: early stopping\n",
            "Attribute:  27\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.7394 - acc: 0.5539 - val_loss: 0.6614 - val_acc: 0.6045\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.66142, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.6385 - acc: 0.6339 - val_loss: 0.6601 - val_acc: 0.6135\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.66142 to 0.66010, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.6167 - acc: 0.6525 - val_loss: 0.6606 - val_acc: 0.6180\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.66010\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.5853 - acc: 0.6879 - val_loss: 0.6588 - val_acc: 0.6130\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.66010 to 0.65885, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.5563 - acc: 0.7085 - val_loss: 0.6738 - val_acc: 0.6175\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.65885\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.5149 - acc: 0.7425 - val_loss: 0.6905 - val_acc: 0.6145\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.65885\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.4682 - acc: 0.7721 - val_loss: 0.7491 - val_acc: 0.6205\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.65885\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.3920 - acc: 0.8194 - val_loss: 0.7931 - val_acc: 0.6105\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.65885\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.3140 - acc: 0.8634 - val_loss: 0.8754 - val_acc: 0.6005\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.65885\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.2361 - acc: 0.9045 - val_loss: 1.0071 - val_acc: 0.6045\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.65885\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.1775 - acc: 0.9326 - val_loss: 1.1404 - val_acc: 0.5950\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.65885\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.1188 - acc: 0.9606 - val_loss: 1.2588 - val_acc: 0.5980\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.65885\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0861 - acc: 0.9733 - val_loss: 1.4177 - val_acc: 0.6025\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.65885\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.0706 - acc: 0.9798 - val_loss: 1.4744 - val_acc: 0.6010\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.65885\n",
            "Epoch 00014: early stopping\n",
            "Attribute:  28\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.6613 - acc: 0.5966 - val_loss: 0.5167 - val_acc: 0.7420\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.51673, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.4612 - acc: 0.7888 - val_loss: 0.4387 - val_acc: 0.8025\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.51673 to 0.43868, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.3880 - acc: 0.8301 - val_loss: 0.4137 - val_acc: 0.8185\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.43868 to 0.41368, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.3468 - acc: 0.8521 - val_loss: 0.4064 - val_acc: 0.8270\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.41368 to 0.40643, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.3043 - acc: 0.8719 - val_loss: 0.4141 - val_acc: 0.8210\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.40643\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.2614 - acc: 0.8927 - val_loss: 0.4356 - val_acc: 0.8140\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.40643\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.2231 - acc: 0.9102 - val_loss: 0.4334 - val_acc: 0.8210\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.40643\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.1815 - acc: 0.9289 - val_loss: 0.4962 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.40643\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.1318 - acc: 0.9504 - val_loss: 0.5237 - val_acc: 0.8150\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.40643\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.1059 - acc: 0.9636 - val_loss: 0.5605 - val_acc: 0.8200\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.40643\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0834 - acc: 0.9700 - val_loss: 0.6397 - val_acc: 0.8190\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.40643\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0692 - acc: 0.9774 - val_loss: 0.6383 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.40643\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0473 - acc: 0.9843 - val_loss: 0.6676 - val_acc: 0.8210\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.40643\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.0338 - acc: 0.9899 - val_loss: 0.7320 - val_acc: 0.8195\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.40643\n",
            "Epoch 00014: early stopping\n",
            "Attribute:  29\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.6670 - acc: 0.6370 - val_loss: 0.5437 - val_acc: 0.7335\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54368, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.4580 - acc: 0.7865 - val_loss: 0.5423 - val_acc: 0.7425\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54368 to 0.54235, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.3409 - acc: 0.8588 - val_loss: 0.3689 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.54235 to 0.36888, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.2827 - acc: 0.8832 - val_loss: 0.3417 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.36888 to 0.34173, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.2465 - acc: 0.9011 - val_loss: 0.3006 - val_acc: 0.8675\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.34173 to 0.30064, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.2079 - acc: 0.9194 - val_loss: 0.3740 - val_acc: 0.8445\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.30064\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.1665 - acc: 0.9378 - val_loss: 0.3517 - val_acc: 0.8535\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.30064\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.1322 - acc: 0.9500 - val_loss: 0.3885 - val_acc: 0.8475\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.30064\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.0968 - acc: 0.9644 - val_loss: 0.4292 - val_acc: 0.8535\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.30064\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.0745 - acc: 0.9753 - val_loss: 0.6343 - val_acc: 0.8055\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.30064\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0567 - acc: 0.9800 - val_loss: 0.6023 - val_acc: 0.8175\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.30064\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0394 - acc: 0.9874 - val_loss: 0.5520 - val_acc: 0.8365\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.30064\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0301 - acc: 0.9916 - val_loss: 0.5746 - val_acc: 0.8380\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.30064\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.0270 - acc: 0.9924 - val_loss: 0.6668 - val_acc: 0.8305\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.30064\n",
            "Epoch 15/50\n",
            " - 12s - loss: 0.0202 - acc: 0.9939 - val_loss: 0.5832 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.30064\n",
            "Epoch 00015: early stopping\n",
            "Attribute:  30\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.6866 - acc: 0.6580 - val_loss: 0.5890 - val_acc: 0.6865\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.58899, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.4193 - acc: 0.8058 - val_loss: 0.3477 - val_acc: 0.8510\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.58899 to 0.34770, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 12s - loss: 0.3000 - acc: 0.8750 - val_loss: 0.2854 - val_acc: 0.8735\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.34770 to 0.28539, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.2462 - acc: 0.8981 - val_loss: 0.2741 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.28539 to 0.27411, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 12s - loss: 0.1984 - acc: 0.9213 - val_loss: 0.2200 - val_acc: 0.9130\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.27411 to 0.21997, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 12s - loss: 0.1659 - acc: 0.9341 - val_loss: 0.2606 - val_acc: 0.8960\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.21997\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.1323 - acc: 0.9505 - val_loss: 0.2373 - val_acc: 0.9125\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.21997\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.1049 - acc: 0.9618 - val_loss: 0.2454 - val_acc: 0.9130\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.21997\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.0903 - acc: 0.9666 - val_loss: 0.2886 - val_acc: 0.9055\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.21997\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.0699 - acc: 0.9736 - val_loss: 0.3008 - val_acc: 0.9070\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.21997\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0522 - acc: 0.9828 - val_loss: 0.2834 - val_acc: 0.9070\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.21997\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0417 - acc: 0.9868 - val_loss: 0.2785 - val_acc: 0.9160\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.21997\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0317 - acc: 0.9898 - val_loss: 0.3742 - val_acc: 0.8995\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.21997\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.0289 - acc: 0.9908 - val_loss: 0.3116 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.21997\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.0212 - acc: 0.9934 - val_loss: 0.3509 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.21997\n",
            "Epoch 00015: early stopping\n",
            "Attribute:  31\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.7226 - acc: 0.6209 - val_loss: 0.4971 - val_acc: 0.7880\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.49707, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.4492 - acc: 0.7876 - val_loss: 0.3874 - val_acc: 0.8365\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.49707 to 0.38740, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.3859 - acc: 0.8270 - val_loss: 0.3525 - val_acc: 0.8595\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.38740 to 0.35249, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.3584 - acc: 0.8406 - val_loss: 0.3717 - val_acc: 0.8440\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.35249\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.3263 - acc: 0.8575 - val_loss: 0.3834 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.35249\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.3031 - acc: 0.8665 - val_loss: 0.3574 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.35249\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.2767 - acc: 0.8820 - val_loss: 0.3940 - val_acc: 0.8225\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.35249\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.2448 - acc: 0.8924 - val_loss: 0.3292 - val_acc: 0.8650\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.35249 to 0.32922, saving model to model.weights.best.hdf5\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.2233 - acc: 0.9058 - val_loss: 0.3824 - val_acc: 0.8395\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.32922\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.1950 - acc: 0.9200 - val_loss: 0.3764 - val_acc: 0.8470\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.32922\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.1638 - acc: 0.9351 - val_loss: 0.3633 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.32922\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.1432 - acc: 0.9450 - val_loss: 0.4028 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.32922\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.1239 - acc: 0.9506 - val_loss: 0.4221 - val_acc: 0.8420\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.32922\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.1027 - acc: 0.9606 - val_loss: 0.4013 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.32922\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.0772 - acc: 0.9735 - val_loss: 0.4422 - val_acc: 0.8480\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.32922\n",
            "Epoch 16/50\n",
            " - 11s - loss: 0.0639 - acc: 0.9790 - val_loss: 0.4644 - val_acc: 0.8485\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.32922\n",
            "Epoch 17/50\n",
            " - 11s - loss: 0.0597 - acc: 0.9795 - val_loss: 0.4903 - val_acc: 0.8390\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.32922\n",
            "Epoch 18/50\n",
            " - 11s - loss: 0.0516 - acc: 0.9821 - val_loss: 0.5290 - val_acc: 0.8345\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.32922\n",
            "Epoch 00018: early stopping\n",
            "Attribute:  32\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.7668 - acc: 0.5530 - val_loss: 0.6560 - val_acc: 0.6795\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.65603, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.6450 - acc: 0.6192 - val_loss: 0.6342 - val_acc: 0.6990\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.65603 to 0.63424, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.5920 - acc: 0.6803 - val_loss: 0.6834 - val_acc: 0.6290\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.63424\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.5169 - acc: 0.7367 - val_loss: 0.7308 - val_acc: 0.6070\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.63424\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.4337 - acc: 0.7963 - val_loss: 0.7332 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.63424\n",
            "Epoch 6/50\n",
            " - 12s - loss: 0.3409 - acc: 0.8526 - val_loss: 1.0037 - val_acc: 0.5335\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.63424\n",
            "Epoch 7/50\n",
            " - 12s - loss: 0.2481 - acc: 0.9011 - val_loss: 1.0399 - val_acc: 0.5805\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.63424\n",
            "Epoch 8/50\n",
            " - 12s - loss: 0.1691 - acc: 0.9384 - val_loss: 1.1624 - val_acc: 0.5805\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.63424\n",
            "Epoch 9/50\n",
            " - 12s - loss: 0.1190 - acc: 0.9589 - val_loss: 1.2820 - val_acc: 0.5905\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.63424\n",
            "Epoch 10/50\n",
            " - 12s - loss: 0.0800 - acc: 0.9739 - val_loss: 1.2764 - val_acc: 0.6560\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.63424\n",
            "Epoch 11/50\n",
            " - 12s - loss: 0.0832 - acc: 0.9734 - val_loss: 1.4913 - val_acc: 0.5810\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.63424\n",
            "Epoch 12/50\n",
            " - 12s - loss: 0.0489 - acc: 0.9851 - val_loss: 1.6550 - val_acc: 0.5950\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.63424\n",
            "Epoch 00012: early stopping\n",
            "Attribute:  33\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.6984 - acc: 0.5335 - val_loss: 0.6670 - val_acc: 0.6040\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.66701, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.6215 - acc: 0.6607 - val_loss: 0.6358 - val_acc: 0.6670\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.66701 to 0.63578, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 12s - loss: 0.5514 - acc: 0.7205 - val_loss: 0.6203 - val_acc: 0.6960\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.63578 to 0.62033, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 12s - loss: 0.4973 - acc: 0.7574 - val_loss: 0.6167 - val_acc: 0.6950\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.62033 to 0.61671, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.4361 - acc: 0.7980 - val_loss: 0.6471 - val_acc: 0.7010\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.61671\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.3793 - acc: 0.8313 - val_loss: 0.6673 - val_acc: 0.7020\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.61671\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.3070 - acc: 0.8721 - val_loss: 0.7465 - val_acc: 0.7020\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.61671\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.2384 - acc: 0.9045 - val_loss: 0.7974 - val_acc: 0.6965\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.61671\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.1893 - acc: 0.9250 - val_loss: 0.9045 - val_acc: 0.7115\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.61671\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.1381 - acc: 0.9510 - val_loss: 0.9922 - val_acc: 0.7025\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.61671\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.1123 - acc: 0.9621 - val_loss: 1.1241 - val_acc: 0.6965\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.61671\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0777 - acc: 0.9733 - val_loss: 1.2238 - val_acc: 0.6965\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.61671\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0589 - acc: 0.9809 - val_loss: 1.2266 - val_acc: 0.7065\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.61671\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.0416 - acc: 0.9870 - val_loss: 1.3726 - val_acc: 0.6920\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.61671\n",
            "Epoch 00014: early stopping\n",
            "Attribute:  34\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.6391 - acc: 0.6483 - val_loss: 0.5984 - val_acc: 0.6810\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59840, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.5543 - acc: 0.7149 - val_loss: 0.5531 - val_acc: 0.7085\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.59840 to 0.55306, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.4693 - acc: 0.7750 - val_loss: 0.5306 - val_acc: 0.7275\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.55306 to 0.53065, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.3944 - acc: 0.8171 - val_loss: 0.5376 - val_acc: 0.7395\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.53065\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.3205 - acc: 0.8594 - val_loss: 0.6046 - val_acc: 0.7255\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.53065\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.2458 - acc: 0.9011 - val_loss: 0.6242 - val_acc: 0.7445\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.53065\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.1784 - acc: 0.9315 - val_loss: 0.6839 - val_acc: 0.7400\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.53065\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.1243 - acc: 0.9562 - val_loss: 0.7281 - val_acc: 0.7415\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.53065\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.0891 - acc: 0.9670 - val_loss: 0.8287 - val_acc: 0.7285\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.53065\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.0712 - acc: 0.9770 - val_loss: 0.8410 - val_acc: 0.7380\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.53065\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0580 - acc: 0.9826 - val_loss: 0.9388 - val_acc: 0.7365\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.53065\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0412 - acc: 0.9876 - val_loss: 1.0289 - val_acc: 0.7415\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.53065\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0391 - acc: 0.9886 - val_loss: 0.9654 - val_acc: 0.7460\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.53065\n",
            "Epoch 00013: early stopping\n",
            "Attribute:  35\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.5104 - acc: 0.7395 - val_loss: 0.2268 - val_acc: 0.9215\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.22676, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.2066 - acc: 0.9231 - val_loss: 0.2655 - val_acc: 0.9060\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.22676\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.1404 - acc: 0.9470 - val_loss: 0.1546 - val_acc: 0.9490\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.22676 to 0.15456, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.1062 - acc: 0.9594 - val_loss: 0.1954 - val_acc: 0.9300\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.15456\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.0716 - acc: 0.9758 - val_loss: 0.2150 - val_acc: 0.9290\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.15456\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.0566 - acc: 0.9806 - val_loss: 0.1846 - val_acc: 0.9400\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.15456\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.0393 - acc: 0.9875 - val_loss: 0.1893 - val_acc: 0.9390\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.15456\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.0252 - acc: 0.9920 - val_loss: 0.2068 - val_acc: 0.9400\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.15456\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.0175 - acc: 0.9949 - val_loss: 0.1974 - val_acc: 0.9505\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.15456\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.0128 - acc: 0.9964 - val_loss: 0.2338 - val_acc: 0.9470\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.15456\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0101 - acc: 0.9970 - val_loss: 0.2398 - val_acc: 0.9395\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.15456\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0075 - acc: 0.9979 - val_loss: 0.2856 - val_acc: 0.9320\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.15456\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0067 - acc: 0.9983 - val_loss: 0.3041 - val_acc: 0.9305\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.15456\n",
            "Epoch 00013: early stopping\n",
            "Attribute:  36\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.6724 - acc: 0.6855 - val_loss: 0.4512 - val_acc: 0.8015\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45121, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.4100 - acc: 0.8190 - val_loss: 0.4203 - val_acc: 0.8175\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.45121 to 0.42034, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 12s - loss: 0.3414 - acc: 0.8557 - val_loss: 0.3317 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.42034 to 0.33166, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 12s - loss: 0.2749 - acc: 0.8902 - val_loss: 0.4062 - val_acc: 0.8355\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.33166\n",
            "Epoch 5/50\n",
            " - 12s - loss: 0.2322 - acc: 0.9066 - val_loss: 0.3138 - val_acc: 0.8730\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.33166 to 0.31380, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 12s - loss: 0.1941 - acc: 0.9213 - val_loss: 0.3169 - val_acc: 0.8745\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.31380\n",
            "Epoch 7/50\n",
            " - 12s - loss: 0.1581 - acc: 0.9375 - val_loss: 0.3242 - val_acc: 0.8780\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.31380\n",
            "Epoch 8/50\n",
            " - 12s - loss: 0.1306 - acc: 0.9521 - val_loss: 0.3849 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.31380\n",
            "Epoch 9/50\n",
            " - 12s - loss: 0.0975 - acc: 0.9654 - val_loss: 0.4065 - val_acc: 0.8600\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.31380\n",
            "Epoch 10/50\n",
            " - 12s - loss: 0.0723 - acc: 0.9759 - val_loss: 0.3878 - val_acc: 0.8730\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.31380\n",
            "Epoch 11/50\n",
            " - 12s - loss: 0.0650 - acc: 0.9764 - val_loss: 0.3824 - val_acc: 0.8800\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.31380\n",
            "Epoch 12/50\n",
            " - 12s - loss: 0.0499 - acc: 0.9844 - val_loss: 0.3928 - val_acc: 0.8790\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.31380\n",
            "Epoch 13/50\n",
            " - 12s - loss: 0.0319 - acc: 0.9911 - val_loss: 0.4178 - val_acc: 0.8745\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.31380\n",
            "Epoch 14/50\n",
            " - 12s - loss: 0.0309 - acc: 0.9905 - val_loss: 0.4924 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.31380\n",
            "Epoch 15/50\n",
            " - 12s - loss: 0.0233 - acc: 0.9934 - val_loss: 0.5395 - val_acc: 0.8660\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.31380\n",
            "Epoch 00015: early stopping\n",
            "Attribute:  37\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 12s - loss: 0.6198 - acc: 0.6765 - val_loss: 0.5980 - val_acc: 0.6720\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59802, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 12s - loss: 0.5196 - acc: 0.7375 - val_loss: 0.5758 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.59802 to 0.57580, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 12s - loss: 0.4415 - acc: 0.7849 - val_loss: 0.6198 - val_acc: 0.6760\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.57580\n",
            "Epoch 4/50\n",
            " - 12s - loss: 0.3478 - acc: 0.8469 - val_loss: 0.6692 - val_acc: 0.6730\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.57580\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.2416 - acc: 0.9052 - val_loss: 0.7867 - val_acc: 0.6720\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.57580\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.1579 - acc: 0.9411 - val_loss: 1.0076 - val_acc: 0.6495\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.57580\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.0940 - acc: 0.9686 - val_loss: 1.1752 - val_acc: 0.6545\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.57580\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.0652 - acc: 0.9814 - val_loss: 1.2675 - val_acc: 0.6490\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.57580\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.0525 - acc: 0.9851 - val_loss: 1.2564 - val_acc: 0.6665\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.57580\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.0446 - acc: 0.9881 - val_loss: 1.4103 - val_acc: 0.6510\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.57580\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0345 - acc: 0.9913 - val_loss: 1.3736 - val_acc: 0.6605\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.57580\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0261 - acc: 0.9926 - val_loss: 1.4391 - val_acc: 0.6645\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.57580\n",
            "Epoch 00012: early stopping\n",
            "Attribute:  38\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.4935 - acc: 0.7622 - val_loss: 0.3385 - val_acc: 0.8425\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.33853, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.2394 - acc: 0.9006 - val_loss: 0.3622 - val_acc: 0.8505\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.33853\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.1741 - acc: 0.9301 - val_loss: 0.3195 - val_acc: 0.8765\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.33853 to 0.31948, saving model to model.weights.best.hdf5\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.1134 - acc: 0.9590 - val_loss: 0.3048 - val_acc: 0.8890\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.31948 to 0.30483, saving model to model.weights.best.hdf5\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.0714 - acc: 0.9744 - val_loss: 0.4274 - val_acc: 0.8560\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.30483\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.0431 - acc: 0.9853 - val_loss: 0.4561 - val_acc: 0.8820\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.30483\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.0275 - acc: 0.9929 - val_loss: 0.4479 - val_acc: 0.8870\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.30483\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.0212 - acc: 0.9941 - val_loss: 0.5061 - val_acc: 0.8820\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.30483\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.0164 - acc: 0.9959 - val_loss: 0.4756 - val_acc: 0.8935\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.30483\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.0101 - acc: 0.9969 - val_loss: 0.4896 - val_acc: 0.8880\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.30483\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.0128 - acc: 0.9968 - val_loss: 0.5566 - val_acc: 0.8830\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.30483\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.0072 - acc: 0.9983 - val_loss: 0.6152 - val_acc: 0.8765\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.30483\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.0075 - acc: 0.9983 - val_loss: 0.7584 - val_acc: 0.8545\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.30483\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.0067 - acc: 0.9984 - val_loss: 0.7959 - val_acc: 0.8490\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.30483\n",
            "Epoch 00014: early stopping\n",
            "Attribute:  39\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/50\n",
            " - 11s - loss: 0.7659 - acc: 0.6173 - val_loss: 0.6083 - val_acc: 0.6880\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.60831, saving model to model.weights.best.hdf5\n",
            "Epoch 2/50\n",
            " - 11s - loss: 0.5921 - acc: 0.6891 - val_loss: 0.5185 - val_acc: 0.7730\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.60831 to 0.51852, saving model to model.weights.best.hdf5\n",
            "Epoch 3/50\n",
            " - 11s - loss: 0.5615 - acc: 0.7102 - val_loss: 0.5515 - val_acc: 0.7390\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.51852\n",
            "Epoch 4/50\n",
            " - 11s - loss: 0.5413 - acc: 0.7256 - val_loss: 0.6356 - val_acc: 0.6445\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.51852\n",
            "Epoch 5/50\n",
            " - 11s - loss: 0.5176 - acc: 0.7375 - val_loss: 0.5056 - val_acc: 0.7760\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.51852 to 0.50561, saving model to model.weights.best.hdf5\n",
            "Epoch 6/50\n",
            " - 11s - loss: 0.4870 - acc: 0.7621 - val_loss: 0.5171 - val_acc: 0.7655\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.50561\n",
            "Epoch 7/50\n",
            " - 11s - loss: 0.4608 - acc: 0.7752 - val_loss: 0.5078 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.50561\n",
            "Epoch 8/50\n",
            " - 11s - loss: 0.4281 - acc: 0.7988 - val_loss: 0.4952 - val_acc: 0.7655\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.50561 to 0.49522, saving model to model.weights.best.hdf5\n",
            "Epoch 9/50\n",
            " - 11s - loss: 0.3879 - acc: 0.8210 - val_loss: 0.6055 - val_acc: 0.7165\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.49522\n",
            "Epoch 10/50\n",
            " - 11s - loss: 0.3579 - acc: 0.8420 - val_loss: 0.5276 - val_acc: 0.7680\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.49522\n",
            "Epoch 11/50\n",
            " - 11s - loss: 0.3014 - acc: 0.8704 - val_loss: 0.5390 - val_acc: 0.7605\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.49522\n",
            "Epoch 12/50\n",
            " - 11s - loss: 0.2551 - acc: 0.8920 - val_loss: 0.8491 - val_acc: 0.6535\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.49522\n",
            "Epoch 13/50\n",
            " - 11s - loss: 0.2167 - acc: 0.9127 - val_loss: 0.6933 - val_acc: 0.7370\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.49522\n",
            "Epoch 14/50\n",
            " - 11s - loss: 0.1728 - acc: 0.9278 - val_loss: 0.8094 - val_acc: 0.7035\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.49522\n",
            "Epoch 15/50\n",
            " - 11s - loss: 0.1338 - acc: 0.9502 - val_loss: 0.7809 - val_acc: 0.7380\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.49522\n",
            "Epoch 16/50\n",
            " - 12s - loss: 0.1111 - acc: 0.9591 - val_loss: 1.0495 - val_acc: 0.6730\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.49522\n",
            "Epoch 17/50\n",
            " - 12s - loss: 0.0855 - acc: 0.9709 - val_loss: 0.9863 - val_acc: 0.7080\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.49522\n",
            "Epoch 18/50\n",
            " - 12s - loss: 0.0714 - acc: 0.9765 - val_loss: 1.0013 - val_acc: 0.7310\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.49522\n",
            "Epoch 00018: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9mFrveu1zuV4",
        "colab_type": "code",
        "outputId": "98da2a61-2c0f-48df-aea2-7e9e10938810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json\t\t\t model18.h5  model34.h5\n",
            "celeba-dataset.zip\t\t model19.h5  model35.h5\n",
            "img_align_celeba\t\t model1.h5   model36.h5\n",
            "img_align_celeba.zip\t\t model20.h5  model37.h5\n",
            "kaggle.json\t\t\t model21.h5  model38.h5\n",
            "list_attr_celeba.csv\t\t model22.h5  model39.h5\n",
            "list_bbox_celeba.csv\t\t model23.h5  model3.h5\n",
            "list_eval_partition.csv\t\t model24.h5  model4.h5\n",
            "list_landmarks_align_celeba.csv  model25.h5  model5.h5\n",
            "model0.h5\t\t\t model26.h5  model6.h5\n",
            "model10.h5\t\t\t model27.h5  model7.h5\n",
            "model11.h5\t\t\t model28.h5  model8.h5\n",
            "model12.h5\t\t\t model29.h5  model9.h5\n",
            "model13.h5\t\t\t model2.h5   model.weights.best.hdf5\n",
            "model14.h5\t\t\t model30.h5  sample_data\n",
            "model15.h5\t\t\t model31.h5  train_data.pkl\n",
            "model16.h5\t\t\t model32.h5\n",
            "model17.h5\t\t\t model33.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sjgTYpHOC4_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tl4bdZcp16p-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZIkUh3ah7P4I",
        "colab_type": "code",
        "outputId": "26c18f0d-0a99-41ee-cbaf-4865ff71ee74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "cell_type": "code",
      "source": [
        "model_ids=[]\n",
        "for i in range(40):\n",
        "  uploaded = drive.CreateFile({'title': 'model'+str(i)+'.h5'})\n",
        "  uploaded.SetContentFile('model'+str(i)+'.h5')\n",
        "  uploaded.Upload()\n",
        "  print('Uploaded file with ID {}'.format(uploaded.get('id')))\n",
        "  model_ids.append(uploaded.get('id'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1XSRp-Jkv3lcYAzW8L35TmpPbLMrJwSbF\n",
            "Uploaded file with ID 1waN3tGQ7FtMGBEeKBnJpZJe9ljV5rAux\n",
            "Uploaded file with ID 1bo9wOvwh-dcowIZTjzsX_Dp2hsuZDfUu\n",
            "Uploaded file with ID 14d-uioK4rEkuCVFH44b0qzE_FMp3bfCT\n",
            "Uploaded file with ID 1Kbkdw6fvNbt56f2D18jLIDYFlDYzf7N2\n",
            "Uploaded file with ID 12WzFUdKrWrhNe8J8AX8sa53bXBcUd6x-\n",
            "Uploaded file with ID 18YmOyz6RKc4C9_BWL68VdK2-KfFOgNWh\n",
            "Uploaded file with ID 1GbpNTbL9HsyzdUpoE4v6kebYeeA9t3AN\n",
            "Uploaded file with ID 1zRWGoSUI7-9nhn_DHuoyKmHBQEKOlIiJ\n",
            "Uploaded file with ID 1xwU2bTSODOw9jKR0gMqvPryeHW-t2Qgu\n",
            "Uploaded file with ID 1z78kLf6Wi29xT9lcqjHFruwenoCRlgOp\n",
            "Uploaded file with ID 1-NE6H807RnZwKtyU2lT_t8CPiQoQ03pa\n",
            "Uploaded file with ID 1yzK12oUbyU2I-9fKohapt0TqV00BOfaC\n",
            "Uploaded file with ID 14dlShTTwMq72mvibMd-SUVnsoOV3Ev3s\n",
            "Uploaded file with ID 1WhBLqGDgE_a1c4KhlM0HzAqd7wuEmdD1\n",
            "Uploaded file with ID 1Fpfny8nR8WqyElnWBGBDaiXQmW3c-pJ9\n",
            "Uploaded file with ID 1jtL2sPXf7oJ0pvsZcgbxOiimyJKIpZc7\n",
            "Uploaded file with ID 1tkneeN4FFMEIKNhIJwleaFvEdaE5Yact\n",
            "Uploaded file with ID 1QBqYJ3ptkeVGytKhOn7l_kCA2CE0eAzD\n",
            "Uploaded file with ID 1mV09hTJCNhNwvPHHXlJipZGXGeHScNsl\n",
            "Uploaded file with ID 1CsYs-3NLunT-cbzvv0seB8mHosh2gSxB\n",
            "Uploaded file with ID 1I6Yx7IYtSvz2cOEGe90soHyKzScsJM4-\n",
            "Uploaded file with ID 12rQfFlHZ83gFT0pSDw8siZJgDvMGpPpF\n",
            "Uploaded file with ID 1rwDMi2cSuRR0lO3dilmDc0n3FOvJkNaE\n",
            "Uploaded file with ID 1dcidicqXoEGNG4H0d1TfuX2q5ibzDMPP\n",
            "Uploaded file with ID 1pHJEyvDg660RA7jLaOvkmt26tYU9e5JV\n",
            "Uploaded file with ID 1JnAUosbh4MNZ229a2u1SHR7T3vIybDJO\n",
            "Uploaded file with ID 1_YXPw6P8SI11uPur3s_e_byVO-yermYS\n",
            "Uploaded file with ID 1mqaHyCt-9Di2c6VayYo0OEufP-ALJ0T8\n",
            "Uploaded file with ID 1XD56jkOcniYD2M43CxPg-Pf7jMepFqO-\n",
            "Uploaded file with ID 11dJS_ID32iA84a5Zo6XsduC41kt8karn\n",
            "Uploaded file with ID 1vbdAc5Ww_oh3p1L6_CsV01EJEaH8Fj66\n",
            "Uploaded file with ID 1h-RHWCBzWd--1nd4Jfrr1KkNre42F3oV\n",
            "Uploaded file with ID 1glxr2o3C9cT8lU5gVEy3qHFMz9wxRKOl\n",
            "Uploaded file with ID 1Su586UP98MwwrnPhMYqtT3VCdKi7TMKk\n",
            "Uploaded file with ID 1cMI5IVR_T6Qj46UYIcXC5A_HHV2EJzuI\n",
            "Uploaded file with ID 1VmqAERbnClnjspYoumMt61gMLkW8lANI\n",
            "Uploaded file with ID 12VIRnsPdzq2luyIBBSNkqsn2UbjkST-I\n",
            "Uploaded file with ID 1hrktPt6hru-Hw3tPyuR8PKbIZUAWr-ES\n",
            "Uploaded file with ID 1aPmEsatLr9Q8WYJxbBkaYboCz2cEbv9Q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKOapGbw8Rlr",
        "colab_type": "code",
        "outputId": "cdad277a-fc34-4ee7-8136-0834dee3d302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "cell_type": "code",
      "source": [
        "model_ids"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1XSRp-Jkv3lcYAzW8L35TmpPbLMrJwSbF',\n",
              " '1waN3tGQ7FtMGBEeKBnJpZJe9ljV5rAux',\n",
              " '1bo9wOvwh-dcowIZTjzsX_Dp2hsuZDfUu',\n",
              " '14d-uioK4rEkuCVFH44b0qzE_FMp3bfCT',\n",
              " '1Kbkdw6fvNbt56f2D18jLIDYFlDYzf7N2',\n",
              " '12WzFUdKrWrhNe8J8AX8sa53bXBcUd6x-',\n",
              " '18YmOyz6RKc4C9_BWL68VdK2-KfFOgNWh',\n",
              " '1GbpNTbL9HsyzdUpoE4v6kebYeeA9t3AN',\n",
              " '1zRWGoSUI7-9nhn_DHuoyKmHBQEKOlIiJ',\n",
              " '1xwU2bTSODOw9jKR0gMqvPryeHW-t2Qgu',\n",
              " '1z78kLf6Wi29xT9lcqjHFruwenoCRlgOp',\n",
              " '1-NE6H807RnZwKtyU2lT_t8CPiQoQ03pa',\n",
              " '1yzK12oUbyU2I-9fKohapt0TqV00BOfaC',\n",
              " '14dlShTTwMq72mvibMd-SUVnsoOV3Ev3s',\n",
              " '1WhBLqGDgE_a1c4KhlM0HzAqd7wuEmdD1',\n",
              " '1Fpfny8nR8WqyElnWBGBDaiXQmW3c-pJ9',\n",
              " '1jtL2sPXf7oJ0pvsZcgbxOiimyJKIpZc7',\n",
              " '1tkneeN4FFMEIKNhIJwleaFvEdaE5Yact',\n",
              " '1QBqYJ3ptkeVGytKhOn7l_kCA2CE0eAzD',\n",
              " '1mV09hTJCNhNwvPHHXlJipZGXGeHScNsl',\n",
              " '1CsYs-3NLunT-cbzvv0seB8mHosh2gSxB',\n",
              " '1I6Yx7IYtSvz2cOEGe90soHyKzScsJM4-',\n",
              " '12rQfFlHZ83gFT0pSDw8siZJgDvMGpPpF',\n",
              " '1rwDMi2cSuRR0lO3dilmDc0n3FOvJkNaE',\n",
              " '1dcidicqXoEGNG4H0d1TfuX2q5ibzDMPP',\n",
              " '1pHJEyvDg660RA7jLaOvkmt26tYU9e5JV',\n",
              " '1JnAUosbh4MNZ229a2u1SHR7T3vIybDJO',\n",
              " '1_YXPw6P8SI11uPur3s_e_byVO-yermYS',\n",
              " '1mqaHyCt-9Di2c6VayYo0OEufP-ALJ0T8',\n",
              " '1XD56jkOcniYD2M43CxPg-Pf7jMepFqO-',\n",
              " '11dJS_ID32iA84a5Zo6XsduC41kt8karn',\n",
              " '1vbdAc5Ww_oh3p1L6_CsV01EJEaH8Fj66',\n",
              " '1h-RHWCBzWd--1nd4Jfrr1KkNre42F3oV',\n",
              " '1glxr2o3C9cT8lU5gVEy3qHFMz9wxRKOl',\n",
              " '1Su586UP98MwwrnPhMYqtT3VCdKi7TMKk',\n",
              " '1cMI5IVR_T6Qj46UYIcXC5A_HHV2EJzuI',\n",
              " '1VmqAERbnClnjspYoumMt61gMLkW8lANI',\n",
              " '12VIRnsPdzq2luyIBBSNkqsn2UbjkST-I',\n",
              " '1hrktPt6hru-Hw3tPyuR8PKbIZUAWr-ES',\n",
              " '1aPmEsatLr9Q8WYJxbBkaYboCz2cEbv9Q']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "T_W4Sk3J9mbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('models.pkl', 'wb') as f:  \n",
        "    pickle.dump([model_ids], f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0YeMmEGJ-WEh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('models.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bZIJL3wRXrG",
        "colab_type": "code",
        "outputId": "4d56d973-b99c-4f1b-9e0d-a9194db9dbf9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-91a624b7-8943-46d0-a37d-649159cff866\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-91a624b7-8943-46d0-a37d-649159cff866\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving models.pkl to models.pkl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'models.pkl': b'\\x80\\x03]q\\x00]q\\x01(X!\\x00\\x00\\x001XSRp-Jkv3lcYAzW8L35TmpPbLMrJwSbFq\\x02X!\\x00\\x00\\x001waN3tGQ7FtMGBEeKBnJpZJe9ljV5rAuxq\\x03X!\\x00\\x00\\x001bo9wOvwh-dcowIZTjzsX_Dp2hsuZDfUuq\\x04X!\\x00\\x00\\x0014d-uioK4rEkuCVFH44b0qzE_FMp3bfCTq\\x05X!\\x00\\x00\\x001Kbkdw6fvNbt56f2D18jLIDYFlDYzf7N2q\\x06X!\\x00\\x00\\x0012WzFUdKrWrhNe8J8AX8sa53bXBcUd6x-q\\x07X!\\x00\\x00\\x0018YmOyz6RKc4C9_BWL68VdK2-KfFOgNWhq\\x08X!\\x00\\x00\\x001GbpNTbL9HsyzdUpoE4v6kebYeeA9t3ANq\\tX!\\x00\\x00\\x001zRWGoSUI7-9nhn_DHuoyKmHBQEKOlIiJq\\nX!\\x00\\x00\\x001xwU2bTSODOw9jKR0gMqvPryeHW-t2Qguq\\x0bX!\\x00\\x00\\x001z78kLf6Wi29xT9lcqjHFruwenoCRlgOpq\\x0cX!\\x00\\x00\\x001-NE6H807RnZwKtyU2lT_t8CPiQoQ03paq\\rX!\\x00\\x00\\x001yzK12oUbyU2I-9fKohapt0TqV00BOfaCq\\x0eX!\\x00\\x00\\x0014dlShTTwMq72mvibMd-SUVnsoOV3Ev3sq\\x0fX!\\x00\\x00\\x001WhBLqGDgE_a1c4KhlM0HzAqd7wuEmdD1q\\x10X!\\x00\\x00\\x001Fpfny8nR8WqyElnWBGBDaiXQmW3c-pJ9q\\x11X!\\x00\\x00\\x001jtL2sPXf7oJ0pvsZcgbxOiimyJKIpZc7q\\x12X!\\x00\\x00\\x001tkneeN4FFMEIKNhIJwleaFvEdaE5Yactq\\x13X!\\x00\\x00\\x001QBqYJ3ptkeVGytKhOn7l_kCA2CE0eAzDq\\x14X!\\x00\\x00\\x001mV09hTJCNhNwvPHHXlJipZGXGeHScNslq\\x15X!\\x00\\x00\\x001CsYs-3NLunT-cbzvv0seB8mHosh2gSxBq\\x16X!\\x00\\x00\\x001I6Yx7IYtSvz2cOEGe90soHyKzScsJM4-q\\x17X!\\x00\\x00\\x0012rQfFlHZ83gFT0pSDw8siZJgDvMGpPpFq\\x18X!\\x00\\x00\\x001rwDMi2cSuRR0lO3dilmDc0n3FOvJkNaEq\\x19X!\\x00\\x00\\x001dcidicqXoEGNG4H0d1TfuX2q5ibzDMPPq\\x1aX!\\x00\\x00\\x001pHJEyvDg660RA7jLaOvkmt26tYU9e5JVq\\x1bX!\\x00\\x00\\x001JnAUosbh4MNZ229a2u1SHR7T3vIybDJOq\\x1cX!\\x00\\x00\\x001_YXPw6P8SI11uPur3s_e_byVO-yermYSq\\x1dX!\\x00\\x00\\x001mqaHyCt-9Di2c6VayYo0OEufP-ALJ0T8q\\x1eX!\\x00\\x00\\x001XD56jkOcniYD2M43CxPg-Pf7jMepFqO-q\\x1fX!\\x00\\x00\\x0011dJS_ID32iA84a5Zo6XsduC41kt8karnq X!\\x00\\x00\\x001vbdAc5Ww_oh3p1L6_CsV01EJEaH8Fj66q!X!\\x00\\x00\\x001h-RHWCBzWd--1nd4Jfrr1KkNre42F3oVq\"X!\\x00\\x00\\x001glxr2o3C9cT8lU5gVEy3qHFMz9wxRKOlq#X!\\x00\\x00\\x001Su586UP98MwwrnPhMYqtT3VCdKi7TMKkq$X!\\x00\\x00\\x001cMI5IVR_T6Qj46UYIcXC5A_HHV2EJzuIq%X!\\x00\\x00\\x001VmqAERbnClnjspYoumMt61gMLkW8lANIq&X!\\x00\\x00\\x0012VIRnsPdzq2luyIBBSNkqsn2UbjkST-Iq\\'X!\\x00\\x00\\x001hrktPt6hru-Hw3tPyuR8PKbIZUAWr-ESq(X!\\x00\\x00\\x001aPmEsatLr9Q8WYJxbBkaYboCz2cEbv9Qq)ea.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pU0x7sCbR3OA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('models.pkl','rb') as f: \n",
        "   model_ids = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YTnEvKN0R-FN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_ids=model_ids[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fiQhml49SDKE",
        "colab_type": "code",
        "outputId": "9f96f0cc-66ca-41fe-8e55-270b7c7e5f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model_ids[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1XSRp-Jkv3lcYAzW8L35TmpPbLMrJwSbF'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NhcybWSHSltf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4qiJAplQSlt8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oUOV4Xv9THGX",
        "colab_type": "code",
        "outputId": "65d0d3d7-0e5a-446e-c9c8-cc77598242b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(40):\n",
        "  download = drive.CreateFile({'id': model_ids[i]})\n",
        "  download.GetContentFile('model'+str(i)+'.h5')\n",
        "  print(\"file:\",i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file: 0\n",
            "file: 1\n",
            "file: 2\n",
            "file: 3\n",
            "file: 4\n",
            "file: 5\n",
            "file: 6\n",
            "file: 7\n",
            "file: 8\n",
            "file: 9\n",
            "file: 10\n",
            "file: 11\n",
            "file: 12\n",
            "file: 13\n",
            "file: 14\n",
            "file: 15\n",
            "file: 16\n",
            "file: 17\n",
            "file: 18\n",
            "file: 19\n",
            "file: 20\n",
            "file: 21\n",
            "file: 22\n",
            "file: 23\n",
            "file: 24\n",
            "file: 25\n",
            "file: 26\n",
            "file: 27\n",
            "file: 28\n",
            "file: 29\n",
            "file: 30\n",
            "file: 31\n",
            "file: 32\n",
            "file: 33\n",
            "file: 34\n",
            "file: 35\n",
            "file: 36\n",
            "file: 37\n",
            "file: 38\n",
            "file: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PNx4epuUCMY5",
        "outputId": "2bc63599-e303-4a37-f673-bd9591732290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd  \n",
        "\n",
        "data=pd.read_csv('/content/list_attr_celeba.csv')\n",
        "\n",
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 202599 entries, 0 to 202598\n",
            "Data columns (total 41 columns):\n",
            "image_id               202599 non-null object\n",
            "5_o_Clock_Shadow       202599 non-null int64\n",
            "Arched_Eyebrows        202599 non-null int64\n",
            "Attractive             202599 non-null int64\n",
            "Bags_Under_Eyes        202599 non-null int64\n",
            "Bald                   202599 non-null int64\n",
            "Bangs                  202599 non-null int64\n",
            "Big_Lips               202599 non-null int64\n",
            "Big_Nose               202599 non-null int64\n",
            "Black_Hair             202599 non-null int64\n",
            "Blond_Hair             202599 non-null int64\n",
            "Blurry                 202599 non-null int64\n",
            "Brown_Hair             202599 non-null int64\n",
            "Bushy_Eyebrows         202599 non-null int64\n",
            "Chubby                 202599 non-null int64\n",
            "Double_Chin            202599 non-null int64\n",
            "Eyeglasses             202599 non-null int64\n",
            "Goatee                 202599 non-null int64\n",
            "Gray_Hair              202599 non-null int64\n",
            "Heavy_Makeup           202599 non-null int64\n",
            "High_Cheekbones        202599 non-null int64\n",
            "Male                   202599 non-null int64\n",
            "Mouth_Slightly_Open    202599 non-null int64\n",
            "Mustache               202599 non-null int64\n",
            "Narrow_Eyes            202599 non-null int64\n",
            "No_Beard               202599 non-null int64\n",
            "Oval_Face              202599 non-null int64\n",
            "Pale_Skin              202599 non-null int64\n",
            "Pointy_Nose            202599 non-null int64\n",
            "Receding_Hairline      202599 non-null int64\n",
            "Rosy_Cheeks            202599 non-null int64\n",
            "Sideburns              202599 non-null int64\n",
            "Smiling                202599 non-null int64\n",
            "Straight_Hair          202599 non-null int64\n",
            "Wavy_Hair              202599 non-null int64\n",
            "Wearing_Earrings       202599 non-null int64\n",
            "Wearing_Hat            202599 non-null int64\n",
            "Wearing_Lipstick       202599 non-null int64\n",
            "Wearing_Necklace       202599 non-null int64\n",
            "Wearing_Necktie        202599 non-null int64\n",
            "Young                  202599 non-null int64\n",
            "dtypes: int64(40), object(1)\n",
            "memory usage: 63.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eIUPrtP3CMZS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "col_list=data.columns.tolist()\n",
        "col_list.remove('image_id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3BYYwYASjwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "import os.path as path\n",
        "from scipy import misc\n",
        "\n",
        "IMAGE_PATH = '/content/img_align_celeba'\n",
        "file_paths = glob.glob(path.join(IMAGE_PATH, '00*.jpg'))\n",
        "\n",
        "images = [misc.imread(path) for path in file_paths]\n",
        "images = images[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w66eSuttC5ML",
        "colab_type": "code",
        "outputId": "e8f710a7-3e9d-4582-8f94-f3f7bba8447d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "dwYzk4onKNtZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "new_img=[]\n",
        "for i in range(len(images)):\n",
        "    img=cv2.resize(images[i],(128,128))\n",
        "    new_img.append(img)\n",
        "    \n",
        "     \n",
        "images=np.asarray(new_img)\n",
        "images=images/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGNszo24-r9D",
        "colab_type": "code",
        "outputId": "b5cc695d-fe86-47e1-a00d-b9029e3105d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5012
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "s=0\n",
        "t=0\n",
        "acc=[]\n",
        "for j in range(40):\n",
        "  print(\"\\n\\nAttribute: \",j)\n",
        "  print(col_list[j])\n",
        "  labels=[]\n",
        "  for i in range(len(images)):\n",
        "    filename = path.basename(file_paths[i])\n",
        "    index=int(filename.split(\".\")[0])\n",
        "    if data[col_list[j]][index-1]==1:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "  labels=np.asarray(labels)\n",
        "  labels=np_utils.to_categorical(labels,2)\n",
        "  model = load_model('model'+str(j)+'.h5')\n",
        "  print(labels[0])\n",
        "  preds=model.predict(images)\n",
        "  print(preds[0])\n",
        "  preds=np.round(preds, 2)\n",
        "  preds[preds>=0.5] = 1\n",
        "  preds[preds<0.5] = 0\n",
        "  score=np.sum(preds==labels)/(preds==labels).size\n",
        "  print(\"Acc:\", score*100) \n",
        "  acc.append(score*100)\n",
        "  s=s+np.sum(preds==labels)\n",
        "  t=t+(preds==labels).size\n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Attribute:  0\n",
            "5_o_Clock_Shadow\n",
            "[1. 0.]\n",
            "[0.6097931  0.39020693]\n",
            "Acc: 83.38\n",
            "\n",
            "\n",
            "Attribute:  1\n",
            "Arched_Eyebrows\n",
            "[0. 1.]\n",
            "[0.6262065  0.37379348]\n",
            "Acc: 73.09\n",
            "\n",
            "\n",
            "Attribute:  2\n",
            "Attractive\n",
            "[0. 1.]\n",
            "[0.57909346 0.42090654]\n",
            "Acc: 76.55999999999999\n",
            "\n",
            "\n",
            "Attribute:  3\n",
            "Bags_Under_Eyes\n",
            "[1. 0.]\n",
            "[0.96553624 0.03446372]\n",
            "Acc: 71.66\n",
            "\n",
            "\n",
            "Attribute:  4\n",
            "Bald\n",
            "[1. 0.]\n",
            "[9.9955219e-01 4.4783042e-04]\n",
            "Acc: 93.57\n",
            "\n",
            "\n",
            "Attribute:  5\n",
            "Bangs\n",
            "[1. 0.]\n",
            "[9.9984610e-01 1.5388326e-04]\n",
            "Acc: 91.51\n",
            "\n",
            "\n",
            "Attribute:  6\n",
            "Big_Lips\n",
            "[1. 0.]\n",
            "[0.54408026 0.45591974]\n",
            "Acc: 65.02\n",
            "\n",
            "\n",
            "Attribute:  7\n",
            "Big_Nose\n",
            "[1. 0.]\n",
            "[0.9625002  0.03749982]\n",
            "Acc: 72.21\n",
            "\n",
            "\n",
            "Attribute:  8\n",
            "Black_Hair\n",
            "[0. 1.]\n",
            "[0.41527942 0.5847206 ]\n",
            "Acc: 81.22\n",
            "\n",
            "\n",
            "Attribute:  9\n",
            "Blond_Hair\n",
            "[1. 0.]\n",
            "[0.89119744 0.10880256]\n",
            "Acc: 90.05\n",
            "\n",
            "\n",
            "Attribute:  10\n",
            "Blurry\n",
            "[1. 0.]\n",
            "[0.76386464 0.23613538]\n",
            "Acc: 89.28\n",
            "\n",
            "\n",
            "Attribute:  11\n",
            "Brown_Hair\n",
            "[1. 0.]\n",
            "[0.65048283 0.34951714]\n",
            "Acc: 80.12\n",
            "\n",
            "\n",
            "Attribute:  12\n",
            "Bushy_Eyebrows\n",
            "[1. 0.]\n",
            "[0.28984755 0.71015245]\n",
            "Acc: 76.74\n",
            "\n",
            "\n",
            "Attribute:  13\n",
            "Chubby\n",
            "[1. 0.]\n",
            "[0.8883894  0.11161059]\n",
            "Acc: 80.63\n",
            "\n",
            "\n",
            "Attribute:  14\n",
            "Double_Chin\n",
            "[1. 0.]\n",
            "[0.78465754 0.2153425 ]\n",
            "Acc: 78.14\n",
            "\n",
            "\n",
            "Attribute:  15\n",
            "Eyeglasses\n",
            "[1. 0.]\n",
            "[9.9999154e-01 8.4400626e-06]\n",
            "Acc: 94.67\n",
            "\n",
            "\n",
            "Attribute:  16\n",
            "Goatee\n",
            "[1. 0.]\n",
            "[0.20283075 0.79716927]\n",
            "Acc: 73.65\n",
            "\n",
            "\n",
            "Attribute:  17\n",
            "Gray_Hair\n",
            "[1. 0.]\n",
            "[0.9981325  0.00186748]\n",
            "Acc: 85.1\n",
            "\n",
            "\n",
            "Attribute:  18\n",
            "Heavy_Makeup\n",
            "[1. 0.]\n",
            "[0.869907   0.13009302]\n",
            "Acc: 87.1\n",
            "\n",
            "\n",
            "Attribute:  19\n",
            "High_Cheekbones\n",
            "[1. 0.]\n",
            "[0.7497586  0.25024143]\n",
            "Acc: 78.02\n",
            "\n",
            "\n",
            "Attribute:  20\n",
            "Male\n",
            "[1. 0.]\n",
            "[0.9511631  0.04883691]\n",
            "Acc: 93.43\n",
            "\n",
            "\n",
            "Attribute:  21\n",
            "Mouth_Slightly_Open\n",
            "[1. 0.]\n",
            "[0.88850534 0.11149466]\n",
            "Acc: 90.36\n",
            "\n",
            "\n",
            "Attribute:  22\n",
            "Mustache\n",
            "[1. 0.]\n",
            "[0.91530305 0.08469697]\n",
            "Acc: 87.03\n",
            "\n",
            "\n",
            "Attribute:  23\n",
            "Narrow_Eyes\n",
            "[1. 0.]\n",
            "[0.44648805 0.553512  ]\n",
            "Acc: 75.36\n",
            "\n",
            "\n",
            "Attribute:  24\n",
            "No_Beard\n",
            "[0. 1.]\n",
            "[0.88803506 0.11196502]\n",
            "Acc: 85.5\n",
            "\n",
            "\n",
            "Attribute:  25\n",
            "Oval_Face\n",
            "[1. 0.]\n",
            "[0.61260647 0.38739356]\n",
            "Acc: 61.83\n",
            "\n",
            "\n",
            "Attribute:  26\n",
            "Pale_Skin\n",
            "[1. 0.]\n",
            "[0.9790425  0.02095758]\n",
            "Acc: 91.05\n",
            "\n",
            "\n",
            "Attribute:  27\n",
            "Pointy_Nose\n",
            "[1. 0.]\n",
            "[0.33783045 0.6621695 ]\n",
            "Acc: 60.34\n",
            "\n",
            "\n",
            "Attribute:  28\n",
            "Receding_Hairline\n",
            "[1. 0.]\n",
            "[0.80175924 0.19824077]\n",
            "Acc: 77.96\n",
            "\n",
            "\n",
            "Attribute:  29\n",
            "Rosy_Cheeks\n",
            "[1. 0.]\n",
            "[0.99770325 0.00229671]\n",
            "Acc: 88.67\n",
            "\n",
            "\n",
            "Attribute:  30\n",
            "Sideburns\n",
            "[1. 0.]\n",
            "[0.9848365  0.01516343]\n",
            "Acc: 91.7\n",
            "\n",
            "\n",
            "Attribute:  31\n",
            "Smiling\n",
            "[1. 0.]\n",
            "[0.95205414 0.04794586]\n",
            "Acc: 85.11\n",
            "\n",
            "\n",
            "Attribute:  32\n",
            "Straight_Hair\n",
            "[1. 0.]\n",
            "[0.54462665 0.45537335]\n",
            "Acc: 67.97\n",
            "\n",
            "\n",
            "Attribute:  33\n",
            "Wavy_Hair\n",
            "[1. 0.]\n",
            "[0.95799124 0.04200876]\n",
            "Acc: 69.77\n",
            "\n",
            "\n",
            "Attribute:  34\n",
            "Wearing_Earrings\n",
            "[0. 1.]\n",
            "[0.77579117 0.22420883]\n",
            "Acc: 70.41\n",
            "\n",
            "\n",
            "Attribute:  35\n",
            "Wearing_Hat\n",
            "[1. 0.]\n",
            "[0.9882884  0.01171154]\n",
            "Acc: 95.48\n",
            "\n",
            "\n",
            "Attribute:  36\n",
            "Wearing_Lipstick\n",
            "[1. 0.]\n",
            "[0.8622387 0.1377613]\n",
            "Acc: 87.92999999999999\n",
            "\n",
            "\n",
            "Attribute:  37\n",
            "Wearing_Necklace\n",
            "[1. 0.]\n",
            "[0.94772816 0.05227185]\n",
            "Acc: 63.629999999999995\n",
            "\n",
            "\n",
            "Attribute:  38\n",
            "Wearing_Necktie\n",
            "[1. 0.]\n",
            "[9.9997914e-01 2.0899499e-05]\n",
            "Acc: 88.62\n",
            "\n",
            "\n",
            "Attribute:  39\n",
            "Young\n",
            "[0. 1.]\n",
            "[0.29202718 0.7079728 ]\n",
            "Acc: 77.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sfm7ZCv_CyHc",
        "colab_type": "code",
        "outputId": "0fb9c657-1dbe-4efd-ea60-ecebec3c8d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "323121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "4LeFT7Z0PJpz",
        "colab_type": "code",
        "outputId": "cea3bc44-11cf-4964-efa3-4bda6edc3c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "kv_I7PSqPK5Z",
        "colab_type": "code",
        "outputId": "1c6ee236-1429-4759-e0c6-b4a83ddc58f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "s/t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8078025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "xfuhaHANPNIf",
        "colab_type": "code",
        "outputId": "d8681c2b-9dac-4655-cbc5-36cef87657f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83.38,\n",
              " 73.09,\n",
              " 76.55999999999999,\n",
              " 71.66,\n",
              " 93.57,\n",
              " 91.51,\n",
              " 65.02,\n",
              " 72.21,\n",
              " 81.22,\n",
              " 90.05,\n",
              " 89.28,\n",
              " 80.12,\n",
              " 76.74,\n",
              " 80.63,\n",
              " 78.14,\n",
              " 94.67,\n",
              " 73.65,\n",
              " 85.1,\n",
              " 87.1,\n",
              " 78.02,\n",
              " 93.43,\n",
              " 90.36,\n",
              " 87.03,\n",
              " 75.36,\n",
              " 85.5,\n",
              " 61.83,\n",
              " 91.05,\n",
              " 60.34,\n",
              " 77.96,\n",
              " 88.67,\n",
              " 91.7,\n",
              " 85.11,\n",
              " 67.97,\n",
              " 69.77,\n",
              " 70.41,\n",
              " 95.48,\n",
              " 87.92999999999999,\n",
              " 63.629999999999995,\n",
              " 88.62,\n",
              " 77.34]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "Wa79MdsiPQQv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc=np.asarray(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zsn7oRvKPxQE",
        "colab_type": "code",
        "outputId": "438a70d3-c772-4e8d-b786-43ea48dc5e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.sum(acc>=75)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WeyrtCRSWpoU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "import os.path as path\n",
        "from scipy import misc\n",
        "\n",
        "IMAGE_PATH = '/content/img_align_celeba'\n",
        "file_paths = glob.glob(path.join(IMAGE_PATH, '10*.jpg'))\n",
        "file_paths=file_paths[:5000]\n",
        "images = [misc.imread(path) for path in file_paths]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "3fd7e62b-b4f3-4242-a536-c40268c30952",
        "id": "j6BwFAR5Wpov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZpTamDjGWppI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "new_img=[]\n",
        "for i in range(len(images)):\n",
        "    img=cv2.resize(images[i],(128,128))\n",
        "    new_img.append(img)\n",
        "    \n",
        "     \n",
        "images=np.asarray(new_img)\n",
        "images=images/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "8192bfb8-64fa-4ef2-87d1-c9743026dafc",
        "id": "yG8YqLkGWppX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5012
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "s=0\n",
        "t=0\n",
        "acc=[]\n",
        "for j in range(40):\n",
        "  print(\"\\n\\nAttribute: \",j)\n",
        "  print(col_list[j])\n",
        "  labels=[]\n",
        "  for i in range(len(images)):\n",
        "    filename = path.basename(file_paths[i])\n",
        "    index=int(filename.split(\".\")[0])\n",
        "    if data[col_list[j]][index-1]==1:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "  labels=np.asarray(labels)\n",
        "  labels=np_utils.to_categorical(labels,2)\n",
        "  model = load_model('model'+str(j)+'.h5')\n",
        "  print(labels[0])\n",
        "  preds=model.predict(images)\n",
        "  print(preds[0])\n",
        "  preds=np.round(preds, 2)\n",
        "  preds[preds>=0.5] = 1\n",
        "  preds[preds<0.5] = 0\n",
        "  score=np.sum(preds==labels)/(preds==labels).size\n",
        "  print(\"Acc:\", score*100) \n",
        "  acc.append(score*100)\n",
        "  s=s+np.sum(preds==labels)\n",
        "  t=t+(preds==labels).size\n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Attribute:  0\n",
            "5_o_Clock_Shadow\n",
            "[0. 1.]\n",
            "[0.40860742 0.5913926 ]\n",
            "Acc: 82.65\n",
            "\n",
            "\n",
            "Attribute:  1\n",
            "Arched_Eyebrows\n",
            "[1. 0.]\n",
            "[0.95855653 0.04144348]\n",
            "Acc: 73.16\n",
            "\n",
            "\n",
            "Attribute:  2\n",
            "Attractive\n",
            "[1. 0.]\n",
            "[0.7120974 0.2879026]\n",
            "Acc: 75.96000000000001\n",
            "\n",
            "\n",
            "Attribute:  3\n",
            "Bags_Under_Eyes\n",
            "[1. 0.]\n",
            "[0.3905992 0.6094008]\n",
            "Acc: 72.76\n",
            "\n",
            "\n",
            "Attribute:  4\n",
            "Bald\n",
            "[1. 0.]\n",
            "[0.87697244 0.12302753]\n",
            "Acc: 93.7\n",
            "\n",
            "\n",
            "Attribute:  5\n",
            "Bangs\n",
            "[1. 0.]\n",
            "[9.9974650e-01 2.5350926e-04]\n",
            "Acc: 91.34\n",
            "\n",
            "\n",
            "Attribute:  6\n",
            "Big_Lips\n",
            "[1. 0.]\n",
            "[0.697024   0.30297598]\n",
            "Acc: 65.03\n",
            "\n",
            "\n",
            "Attribute:  7\n",
            "Big_Nose\n",
            "[1. 0.]\n",
            "[0.24263915 0.7573608 ]\n",
            "Acc: 72.22\n",
            "\n",
            "\n",
            "Attribute:  8\n",
            "Black_Hair\n",
            "[1. 0.]\n",
            "[0.36715287 0.63284713]\n",
            "Acc: 81.07\n",
            "\n",
            "\n",
            "Attribute:  9\n",
            "Blond_Hair\n",
            "[1. 0.]\n",
            "[9.9990714e-01 9.2904382e-05]\n",
            "Acc: 90.38000000000001\n",
            "\n",
            "\n",
            "Attribute:  10\n",
            "Blurry\n",
            "[1. 0.]\n",
            "[0.9689507  0.03104937]\n",
            "Acc: 89.41\n",
            "\n",
            "\n",
            "Attribute:  11\n",
            "Brown_Hair\n",
            "[1. 0.]\n",
            "[0.983658 0.016342]\n",
            "Acc: 79.91\n",
            "\n",
            "\n",
            "Attribute:  12\n",
            "Bushy_Eyebrows\n",
            "[1. 0.]\n",
            "[0.04335111 0.9566489 ]\n",
            "Acc: 77.37\n",
            "\n",
            "\n",
            "Attribute:  13\n",
            "Chubby\n",
            "[1. 0.]\n",
            "[0.0890566 0.9109434]\n",
            "Acc: 80.2\n",
            "\n",
            "\n",
            "Attribute:  14\n",
            "Double_Chin\n",
            "[1. 0.]\n",
            "[0.0792757  0.92072433]\n",
            "Acc: 78.9\n",
            "\n",
            "\n",
            "Attribute:  15\n",
            "Eyeglasses\n",
            "[1. 0.]\n",
            "[0.9851951  0.01480497]\n",
            "Acc: 94.92\n",
            "\n",
            "\n",
            "Attribute:  16\n",
            "Goatee\n",
            "[1. 0.]\n",
            "[0.13113578 0.86886424]\n",
            "Acc: 73.63\n",
            "\n",
            "\n",
            "Attribute:  17\n",
            "Gray_Hair\n",
            "[1. 0.]\n",
            "[0.65367305 0.34632695]\n",
            "Acc: 85.02\n",
            "\n",
            "\n",
            "Attribute:  18\n",
            "Heavy_Makeup\n",
            "[1. 0.]\n",
            "[9.9999464e-01 5.3126905e-06]\n",
            "Acc: 86.04\n",
            "\n",
            "\n",
            "Attribute:  19\n",
            "High_Cheekbones\n",
            "[1. 0.]\n",
            "[0.74937034 0.25062966]\n",
            "Acc: 77.10000000000001\n",
            "\n",
            "\n",
            "Attribute:  20\n",
            "Male\n",
            "[0. 1.]\n",
            "[0.00488382 0.9951161 ]\n",
            "Acc: 93.66\n",
            "\n",
            "\n",
            "Attribute:  21\n",
            "Mouth_Slightly_Open\n",
            "[0. 1.]\n",
            "[0.27864352 0.7213565 ]\n",
            "Acc: 90.24\n",
            "\n",
            "\n",
            "Attribute:  22\n",
            "Mustache\n",
            "[1. 0.]\n",
            "[0.8039605  0.19603954]\n",
            "Acc: 87.55\n",
            "\n",
            "\n",
            "Attribute:  23\n",
            "Narrow_Eyes\n",
            "[1. 0.]\n",
            "[0.89685065 0.10314932]\n",
            "Acc: 76.33\n",
            "\n",
            "\n",
            "Attribute:  24\n",
            "No_Beard\n",
            "[1. 0.]\n",
            "[0.5345086  0.46549138]\n",
            "Acc: 85.77\n",
            "\n",
            "\n",
            "Attribute:  25\n",
            "Oval_Face\n",
            "[1. 0.]\n",
            "[0.7188682  0.28113177]\n",
            "Acc: 62.519999999999996\n",
            "\n",
            "\n",
            "Attribute:  26\n",
            "Pale_Skin\n",
            "[1. 0.]\n",
            "[0.98727566 0.0127243 ]\n",
            "Acc: 91.56\n",
            "\n",
            "\n",
            "Attribute:  27\n",
            "Pointy_Nose\n",
            "[1. 0.]\n",
            "[0.7637799  0.23622018]\n",
            "Acc: 59.36\n",
            "\n",
            "\n",
            "Attribute:  28\n",
            "Receding_Hairline\n",
            "[1. 0.]\n",
            "[0.9968501  0.00314995]\n",
            "Acc: 79.63\n",
            "\n",
            "\n",
            "Attribute:  29\n",
            "Rosy_Cheeks\n",
            "[1. 0.]\n",
            "[0.9961319  0.00386806]\n",
            "Acc: 89.0\n",
            "\n",
            "\n",
            "Attribute:  30\n",
            "Sideburns\n",
            "[1. 0.]\n",
            "[0.7838373  0.21616265]\n",
            "Acc: 92.97\n",
            "\n",
            "\n",
            "Attribute:  31\n",
            "Smiling\n",
            "[1. 0.]\n",
            "[0.9881827  0.01181725]\n",
            "Acc: 84.89999999999999\n",
            "\n",
            "\n",
            "Attribute:  32\n",
            "Straight_Hair\n",
            "[1. 0.]\n",
            "[0.50581175 0.49418825]\n",
            "Acc: 66.4\n",
            "\n",
            "\n",
            "Attribute:  33\n",
            "Wavy_Hair\n",
            "[1. 0.]\n",
            "[0.9665485  0.03345148]\n",
            "Acc: 68.97999999999999\n",
            "\n",
            "\n",
            "Attribute:  34\n",
            "Wearing_Earrings\n",
            "[1. 0.]\n",
            "[0.9921353  0.00786475]\n",
            "Acc: 71.07\n",
            "\n",
            "\n",
            "Attribute:  35\n",
            "Wearing_Hat\n",
            "[0. 1.]\n",
            "[9.5553833e-06 9.9999046e-01]\n",
            "Acc: 95.28\n",
            "\n",
            "\n",
            "Attribute:  36\n",
            "Wearing_Lipstick\n",
            "[1. 0.]\n",
            "[9.9916875e-01 8.3128887e-04]\n",
            "Acc: 86.53\n",
            "\n",
            "\n",
            "Attribute:  37\n",
            "Wearing_Necklace\n",
            "[1. 0.]\n",
            "[0.97655666 0.02344339]\n",
            "Acc: 62.8\n",
            "\n",
            "\n",
            "Attribute:  38\n",
            "Wearing_Necktie\n",
            "[1. 0.]\n",
            "[0.6041591 0.3958409]\n",
            "Acc: 88.42\n",
            "\n",
            "\n",
            "Attribute:  39\n",
            "Young\n",
            "[0. 1.]\n",
            "[0.58432025 0.41567978]\n",
            "Acc: 78.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "6acc32dd-2bf2-4729-9957-594819657377",
        "id": "BOh-Rb-rWppv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "323193"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "8e6bb166-ce7d-4b01-e6a9-2a010b2d8d20",
        "id": "SLIXhtSSWpqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "59ebd4d1-7b13-4c58-f7ff-97c636012b6e",
        "id": "WgEBElH7Wpql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "s/t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8079825"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "98c71a73-fc38-48a8-c7ce-451a83f030bd",
        "id": "Qh6TQ20mWprf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[82.65,\n",
              " 73.16,\n",
              " 75.96000000000001,\n",
              " 72.76,\n",
              " 93.7,\n",
              " 91.34,\n",
              " 65.03,\n",
              " 72.22,\n",
              " 81.07,\n",
              " 90.38000000000001,\n",
              " 89.41,\n",
              " 79.91,\n",
              " 77.37,\n",
              " 80.2,\n",
              " 78.9,\n",
              " 94.92,\n",
              " 73.63,\n",
              " 85.02,\n",
              " 86.04,\n",
              " 77.10000000000001,\n",
              " 93.66,\n",
              " 90.24,\n",
              " 87.55,\n",
              " 76.33,\n",
              " 85.77,\n",
              " 62.519999999999996,\n",
              " 91.56,\n",
              " 59.36,\n",
              " 79.63,\n",
              " 89.0,\n",
              " 92.97,\n",
              " 84.89999999999999,\n",
              " 66.4,\n",
              " 68.97999999999999,\n",
              " 71.07,\n",
              " 95.28,\n",
              " 86.53,\n",
              " 62.8,\n",
              " 88.42,\n",
              " 78.19]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kqdVF5-nWpr0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc=np.asarray(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "549820b3-2a1d-49c1-8c0a-615076fa83d9",
        "id": "_YL-kOtXWpsA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.sum(acc>=75)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "RyLbBhfxP31j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "feat=[]\n",
        "index=[]\n",
        "for i in range(40):\n",
        "  if acc[i]>=78:\n",
        "    feat.append(col_list[i])\n",
        "    index.append(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LECF7i4udUR_",
        "colab_type": "code",
        "outputId": "7a8947a8-4f59-492e-cc7f-25fdd5cd8685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "cell_type": "code",
      "source": [
        "feat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5_o_Clock_Shadow',\n",
              " 'Bald',\n",
              " 'Bangs',\n",
              " 'Black_Hair',\n",
              " 'Blond_Hair',\n",
              " 'Blurry',\n",
              " 'Brown_Hair',\n",
              " 'Chubby',\n",
              " 'Double_Chin',\n",
              " 'Eyeglasses',\n",
              " 'Gray_Hair',\n",
              " 'Heavy_Makeup',\n",
              " 'Male',\n",
              " 'Mouth_Slightly_Open',\n",
              " 'Mustache',\n",
              " 'No_Beard',\n",
              " 'Pale_Skin',\n",
              " 'Receding_Hairline',\n",
              " 'Rosy_Cheeks',\n",
              " 'Sideburns',\n",
              " 'Smiling',\n",
              " 'Wearing_Hat',\n",
              " 'Wearing_Lipstick',\n",
              " 'Wearing_Necktie',\n",
              " 'Young']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "wM_1WTq_dWaA",
        "colab_type": "code",
        "outputId": "d0c62ff6-6a79-4493-a5d9-c55718b90475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "cell_type": "code",
      "source": [
        "index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 4,\n",
              " 5,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 17,\n",
              " 18,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 24,\n",
              " 26,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 35,\n",
              " 36,\n",
              " 38,\n",
              " 39]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "icdHnVGRdZ4v",
        "colab_type": "code",
        "outputId": "a1735c12-a7ca-4feb-ba43-32ffbdf4dc1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(feat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "tnPP7ayIdjWP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}